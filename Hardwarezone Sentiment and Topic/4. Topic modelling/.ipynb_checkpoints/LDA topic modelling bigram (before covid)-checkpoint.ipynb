{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8297fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim for topic modeling\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import matutils, models\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# word by freq \n",
    "# wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d749a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shawn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import scipy.sparse\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\" , disable=['parser', 'ner'])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca87f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
    "#!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8682b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('beforecovid.csv')\n",
    "# df = df[[\"Date\",\"text\"]]\n",
    "df[\"Year\"] = df[\"Date\"].str.split(\"-\").str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0265c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19    4842\n",
       "17    3447\n",
       "18    3369\n",
       "20    1804\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce1eea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assigned = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b09e8693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>join_clean_text</th>\n",
       "      <th>real_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25-Jan-19</td>\n",
       "      <td>Mine is a 20 years resale flat, was top in 199...</td>\n",
       "      <td>19</td>\n",
       "      <td>Jan</td>\n",
       "      <td>['mine', '20', 'years', 'resale', 'flat', 'top...</td>\n",
       "      <td>mine 20 years resale flat top 1999 got bomb sh...</td>\n",
       "      <td>2019-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25-Jan-19</td>\n",
       "      <td>kimurayuki said:\\r\\nMine is a 20 years resale ...</td>\n",
       "      <td>19</td>\n",
       "      <td>Jan</td>\n",
       "      <td>['kimurayuki', 'mine', '20', 'years', 'resale'...</td>\n",
       "      <td>kimurayuki mine 20 years resale flat top 1999 ...</td>\n",
       "      <td>2019-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18-Jan-19</td>\n",
       "      <td>punggolbto said:\\r\\nAs per the actual HDB mode...</td>\n",
       "      <td>19</td>\n",
       "      <td>Jan</td>\n",
       "      <td>['punggolbto', 'per', 'actual', 'hdb', 'models...</td>\n",
       "      <td>punggolbto per actual hdb models floor plans p...</td>\n",
       "      <td>2019-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19-Oct-18</td>\n",
       "      <td>I have just purchased a resale flat recently w...</td>\n",
       "      <td>18</td>\n",
       "      <td>Oct</td>\n",
       "      <td>['purchased', 'resale', 'flat', 'recently', 'w...</td>\n",
       "      <td>purchased resale flat recently without agent u...</td>\n",
       "      <td>2018-10-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20-Oct-18</td>\n",
       "      <td>Just to add, go to hdb website and check recen...</td>\n",
       "      <td>18</td>\n",
       "      <td>Oct</td>\n",
       "      <td>['add', 'go', 'hdb', 'website', 'check', 'rece...</td>\n",
       "      <td>add go hdb website check recent 6mths resale t...</td>\n",
       "      <td>2018-10-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13457</th>\n",
       "      <td>12-Oct-19</td>\n",
       "      <td>Van Holland Former Toho Mansion Holland Villag...</td>\n",
       "      <td>19</td>\n",
       "      <td>Oct</td>\n",
       "      <td>['van', 'holland', 'former', 'toho', 'mansion'...</td>\n",
       "      <td>van holland former toho mansion holland villag...</td>\n",
       "      <td>2019-10-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13458</th>\n",
       "      <td>13-Oct-19</td>\n",
       "      <td>Another project:\\nFinal 15 units of T.O.P Read...</td>\n",
       "      <td>19</td>\n",
       "      <td>Oct</td>\n",
       "      <td>['another', 'project', 'final', '15', 'units',...</td>\n",
       "      <td>another project final 15 units p ready boutiqu...</td>\n",
       "      <td>2019-10-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13459</th>\n",
       "      <td>13-Oct-19</td>\n",
       "      <td>Zetrio2006 said:\\nAnother project:\\nFinal 15 u...</td>\n",
       "      <td>19</td>\n",
       "      <td>Oct</td>\n",
       "      <td>['zetrio2006', 'another', 'project', 'final', ...</td>\n",
       "      <td>zetrio2006 another project final 15 units p re...</td>\n",
       "      <td>2019-10-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13460</th>\n",
       "      <td>13-Oct-19</td>\n",
       "      <td>bolster said:\\nWow... did you try walking to t...</td>\n",
       "      <td>19</td>\n",
       "      <td>Oct</td>\n",
       "      <td>['bolster', 'wow', 'try', 'walking', 'holland'...</td>\n",
       "      <td>bolster wow try walking holland v mrt really 5...</td>\n",
       "      <td>2019-10-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13461</th>\n",
       "      <td>22-Nov-19</td>\n",
       "      <td>Zetrio2006 said:\\nYes 5 mins. What's wrong?\\n\\...</td>\n",
       "      <td>19</td>\n",
       "      <td>Nov</td>\n",
       "      <td>['zetrio2006', 'yes', '5', 'mins', 'wrong', 'o...</td>\n",
       "      <td>zetrio2006 yes 5 mins wrong oh tried walking m...</td>\n",
       "      <td>2019-11-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13462 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date                                               text Year  \\\n",
       "0      25-Jan-19  Mine is a 20 years resale flat, was top in 199...   19   \n",
       "1      25-Jan-19  kimurayuki said:\\r\\nMine is a 20 years resale ...   19   \n",
       "2      18-Jan-19  punggolbto said:\\r\\nAs per the actual HDB mode...   19   \n",
       "3      19-Oct-18  I have just purchased a resale flat recently w...   18   \n",
       "4      20-Oct-18  Just to add, go to hdb website and check recen...   18   \n",
       "...          ...                                                ...  ...   \n",
       "13457  12-Oct-19  Van Holland Former Toho Mansion Holland Villag...   19   \n",
       "13458  13-Oct-19  Another project:\\nFinal 15 units of T.O.P Read...   19   \n",
       "13459  13-Oct-19  Zetrio2006 said:\\nAnother project:\\nFinal 15 u...   19   \n",
       "13460  13-Oct-19  bolster said:\\nWow... did you try walking to t...   19   \n",
       "13461  22-Nov-19  Zetrio2006 said:\\nYes 5 mins. What's wrong?\\n\\...   19   \n",
       "\n",
       "      Month                                         clean_text  \\\n",
       "0       Jan  ['mine', '20', 'years', 'resale', 'flat', 'top...   \n",
       "1       Jan  ['kimurayuki', 'mine', '20', 'years', 'resale'...   \n",
       "2       Jan  ['punggolbto', 'per', 'actual', 'hdb', 'models...   \n",
       "3       Oct  ['purchased', 'resale', 'flat', 'recently', 'w...   \n",
       "4       Oct  ['add', 'go', 'hdb', 'website', 'check', 'rece...   \n",
       "...     ...                                                ...   \n",
       "13457   Oct  ['van', 'holland', 'former', 'toho', 'mansion'...   \n",
       "13458   Oct  ['another', 'project', 'final', '15', 'units',...   \n",
       "13459   Oct  ['zetrio2006', 'another', 'project', 'final', ...   \n",
       "13460   Oct  ['bolster', 'wow', 'try', 'walking', 'holland'...   \n",
       "13461   Nov  ['zetrio2006', 'yes', '5', 'mins', 'wrong', 'o...   \n",
       "\n",
       "                                         join_clean_text   real_Date  \n",
       "0      mine 20 years resale flat top 1999 got bomb sh...  2019-01-25  \n",
       "1      kimurayuki mine 20 years resale flat top 1999 ...  2019-01-25  \n",
       "2      punggolbto per actual hdb models floor plans p...  2019-01-18  \n",
       "3      purchased resale flat recently without agent u...  2018-10-19  \n",
       "4      add go hdb website check recent 6mths resale t...  2018-10-20  \n",
       "...                                                  ...         ...  \n",
       "13457  van holland former toho mansion holland villag...  2019-10-12  \n",
       "13458  another project final 15 units p ready boutiqu...  2019-10-13  \n",
       "13459  zetrio2006 another project final 15 units p re...  2019-10-13  \n",
       "13460  bolster wow try walking holland v mrt really 5...  2019-10-13  \n",
       "13461  zetrio2006 yes 5 mins wrong oh tried walking m...  2019-11-22  \n",
       "\n",
       "[13462 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_assigned = df_assigned.reset_index(drop=True)\n",
    "df_assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332fbd19",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ea3d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English words\n",
    "\n",
    "# remove punc \n",
    "df_assigned['clean_text']  = df_assigned['join_clean_text'].map(lambda x: re.sub(\"[^A-Za-z0-9]+\",\" \", (x)))\n",
    "# df_assigned['clean_text']  = df_assigned['join_clean_text'].map(lambda x:re.sub(r'[.,\"\\'-?:!;]',\" \", str(x)))\n",
    "\n",
    "\n",
    "# # lower case\n",
    "df_assigned['clean_text']  = df_assigned['clean_text'].apply(lambda x: x.lower())\n",
    "# # tokenize\n",
    "df_assigned['clean_text']  = [word_tokenize(row) for row in df_assigned['clean_text']]\n",
    "# # remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "new_stop_words = ['u','m','lol','condo','said','also',\"click\",\"expand\",\"http\",\"www\",\"com\",\"https\",'from', 'subject', 're', 'edu', 'use']\n",
    "stop_words.extend(new_stop_words)\n",
    "def remove_stopwords(text):\n",
    "    return [w for w in text if w not in stop_words]\n",
    "df_assigned['clean_text']  = df_assigned['clean_text'].apply(lambda x: remove_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "945e4764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mine_20 20_years years_resale resale_flat flat_top top_1999 1999_got got_bomb bomb_shelter shelter_aircon aircon_ledge ledge_noise noise_external external_traffic traffic_supermarket supermarket_loading loading_bay bay_crazyclubx crazyclubx_last last_time time_rent rent_unfortunately unfortunately_bought bought_punggol punggol_house house_thought thought_home home_haiz haiz_end end_peace peace_knew knew_rather rather_rent rent_forever forever_least least_change change_buy buy_wait wait_5 5_years years_sell sell_move move_slamming slamming_doors doors_clearly clearly_heard heard_hdb hdb_stayed stayed_hear hear_noise noise_slamming slamming_doors doors_haiz haiz_noisy noisy_inconsiderate inconsiderate_upstairs upstairs_neighbours neighbours_r r_pain\n",
      "kimurayuki_mine mine_20 20_years years_resale resale_flat flat_top top_1999 1999_got got_bomb bomb_shelter shelter_aircon aircon_ledge ledge_noise noise_external external_traffic traffic_supermarket supermarket_loading loading_bay bay_well well_mine mine_7 7_8 8_years years_old old_resales resales_flat flat_yes yes_home home_heard heard_traffic traffic_besides besides_busy busy_road road_traffic traffic_noise noise_sleep sleep_w w_aircon aircon_noise noise_neighbour neighbour_made made_slammed slammed_doors doors_sometime sometime_1 1_2am 2am_sometimes sometimes_things things_dropped dropped_2 2_3am 3am_think think_timing timing_quiet quiet_time time_normal normal_ppl ppl_sleep sleep_haiz haiz_spore spore_shld shld_law law_inconsiderate inconsiderate_neighbours\n",
      "punggolbto_per per_actual actual_hdb hdb_models models_floor floor_plans plans_potential potential_buildings buildings_north north_current current_bto bto_view view_units units_coney coney_island island_area area_industrial industrial_site site_johor johor_think think_add add_word word_currently currently_higher higher_average average_price price_bto bto_reflects reflects_proximity proximity_punggol punggol_coast coast_mrt mrt_station station_always always_wondering wondering_monkeys monkeys_island island_venture venture_nearest nearest_bto bto_punggol punggol_point point_cove cove_looking looking_food\n",
      "purchased_resale resale_flat flat_recently recently_without without_agent agent_unit unit_currently currently_renovation renovation_1 1_useful useful_source source_information information_hdb hdb_resale resale_seminar seminar_pay pay_fee fee_small small_compared compared_impending impending_purchase purchase_get get_short short_intro intro_notes notes_hdb hdb_officer officer_important important_things things_take take_note note_get get_ask ask_questions questions_interested interested_2 2_follow follow_hdb hdb_resale resale_portal portal_instruction instruction_step step_step step_instructions instructions_unlikely unlikely_go go_wrong wrong_start start_process process_check check_portal portal_hdb hdb_letters letters_3 3_important important_question question_probably probably_timeline timeline_trying trying_search search_online online_find find_useful useful_info info_1st 1st_jan jan_2018 2018_provide provide_experience experience_day day_0 0_obtain obtain_otp otp_sunday sunday_day day_0 0_request request_valuation valuation_hdb hdb_resale resale_portal portal_day day_3 3_hdb hdb_valuation valuation_result result_evening evening_day day_4 4_bank bank_informed informed_prepare prepare_letter letter_offer offer_already already_principle principle_approval approval_day day_9 9_obtained obtained_signed signed_bank bank_loan loan_letter letter_offer offer_slight slight_delay delay_due due_manpower manpower_shortage shortage_probably probably_due due_influx influx_home home_loan loan_application application_newly newly_announced announced_cooling cooling_measure measure_day day_9 9_signed signed_otp otp_day day_10 10_law law_firm firm_informed informed_bank bank_contacted contacted_day day_11 11_sent sent_supporting supporting_documents documents_law law_firm firm_day day_12 12_obtain obtain_solicitor solicitor_letter letter_day day_12 12_submit submit_resale resale_application application_hdb hdb_resale resale_portal portal_inform inform_seller seller_day day_16 16_seller seller_submitted submitted_documents documents_day day_18 18_receive receive_hdb hdb_acceptance acceptance_letter letter_send send_solicitor solicitor_day day_24 24_endorse endorse_hdb hdb_documents documents_day day_24 24_meet meet_laywer laywer_sign sign_documents documents_finalize finalize_finance finance_day day_26 26_sellers sellers_endorsed endorsed_documents documents_day day_33 33_received received_hdb hdb_approval approval_letter letter_wrong wrong_completion completion_date date_sellers sellers_forgot forgot_inform inform_hdb hdb_shift shift_completion completion_date date_later later_date date_sellers sellers_requested requested_shift shift_completion completion_later later_date date_agree agree_extension extension_day day_39 39_received received_confirmation confirmation_hdb hdb_lawyer lawyer_change change_completion completion_date date_day day_81 81_house house_inspection inspection_confirm confirm_vacancy vacancy_day day_82 82_completion completion_pre pre_agreeed agreeed_sellers sellers_could could_day day_71 71_based based_hdb hdb_first first_chosen chosen_date date_4 4_know know_value value_flat flat_number number_things things_1 1_check check_x x_value value_unit unit_visited visited_unit unit_know know_unit unit_number number_go go_srx srx_website website_x x_value value_button button_type type_unit unit_directly directly_tell tell_market market_value value_directly directly_sometimes sometimes_number number_differ differ_srx srx_page page_posted posted_agent agent_happens happens_unit unit_exercise exercise_judgement judgement_one one_accurate accurate_2 2_tracking tracking_units units_regions regions_interested interested_half half_year year_visited visited_quite quite_number number_units units_sold sold_try try_relate relate_price price_sold sold_based based_hdb hdb_resale resale_info info_time time_units units_taken taken_website website_actual actual_conditions conditions_unit unit_3 3_look look_red red_flags flags_units units_visit visit_units units_morning morning_late late_noon noon_night night_4 4_look look_mouldy mouldy_corners corners_careful careful_enough enough_point point_told told_contractor contractor_verified verified_town town_council council_one one_room room_water water_seepage seepage_problem problem_one one_corner corner_end end_town town_council council_take take_care care_external external_wall wall_take take_care care_interior interior_mouldy mouldy_crack crack_wall wall_pay pay_small small_lesson lesson_fee fee_renovation renovation_cost cost_5 5_check check_owners owners_need need_extension extension_would would_advise advise_extension extension_potential potential_problem\n",
      "add_go go_hdb hdb_website website_check check_recent recent_6mths 6mths_resale resale_transaction transaction_find find_market market_price price_area area_real real_selling selling_price price_compared compared_view view_propertyguru propertyguru_srx srx_99homes 99homes_ohmyhome ohmyhome_offer offer_market market_price price_owner owner_negotiate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>join_clean_text</th>\n",
       "      <th>real_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25-Jan-19</td>\n",
       "      <td>Mine is a 20 years resale flat, was top in 199...</td>\n",
       "      <td>19</td>\n",
       "      <td>Jan</td>\n",
       "      <td>[mine_20, 20_years, years_resale, resale_flat,...</td>\n",
       "      <td>mine 20 years resale flat top 1999 got bomb sh...</td>\n",
       "      <td>2019-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25-Jan-19</td>\n",
       "      <td>kimurayuki said:\\r\\nMine is a 20 years resale ...</td>\n",
       "      <td>19</td>\n",
       "      <td>Jan</td>\n",
       "      <td>[kimurayuki_mine, mine_20, 20_years, years_res...</td>\n",
       "      <td>kimurayuki mine 20 years resale flat top 1999 ...</td>\n",
       "      <td>2019-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18-Jan-19</td>\n",
       "      <td>punggolbto said:\\r\\nAs per the actual HDB mode...</td>\n",
       "      <td>19</td>\n",
       "      <td>Jan</td>\n",
       "      <td>[punggolbto_per, per_actual, actual_hdb, hdb_m...</td>\n",
       "      <td>punggolbto per actual hdb models floor plans p...</td>\n",
       "      <td>2019-01-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19-Oct-18</td>\n",
       "      <td>I have just purchased a resale flat recently w...</td>\n",
       "      <td>18</td>\n",
       "      <td>Oct</td>\n",
       "      <td>[purchased_resale, resale_flat, flat_recently,...</td>\n",
       "      <td>purchased resale flat recently without agent u...</td>\n",
       "      <td>2018-10-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20-Oct-18</td>\n",
       "      <td>Just to add, go to hdb website and check recen...</td>\n",
       "      <td>18</td>\n",
       "      <td>Oct</td>\n",
       "      <td>[add_go, go_hdb, hdb_website, website_check, c...</td>\n",
       "      <td>add go hdb website check recent 6mths resale t...</td>\n",
       "      <td>2018-10-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date                                               text Year Month  \\\n",
       "0  25-Jan-19  Mine is a 20 years resale flat, was top in 199...   19   Jan   \n",
       "1  25-Jan-19  kimurayuki said:\\r\\nMine is a 20 years resale ...   19   Jan   \n",
       "2  18-Jan-19  punggolbto said:\\r\\nAs per the actual HDB mode...   19   Jan   \n",
       "3  19-Oct-18  I have just purchased a resale flat recently w...   18   Oct   \n",
       "4  20-Oct-18  Just to add, go to hdb website and check recen...   18   Oct   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  [mine_20, 20_years, years_resale, resale_flat,...   \n",
       "1  [kimurayuki_mine, mine_20, 20_years, years_res...   \n",
       "2  [punggolbto_per, per_actual, actual_hdb, hdb_m...   \n",
       "3  [purchased_resale, resale_flat, flat_recently,...   \n",
       "4  [add_go, go_hdb, hdb_website, website_check, c...   \n",
       "\n",
       "                                     join_clean_text   real_Date  \n",
       "0  mine 20 years resale flat top 1999 got bomb sh...  2019-01-25  \n",
       "1  kimurayuki mine 20 years resale flat top 1999 ...  2019-01-25  \n",
       "2  punggolbto per actual hdb models floor plans p...  2019-01-18  \n",
       "3  purchased resale flat recently without agent u...  2018-10-19  \n",
       "4  add go hdb website check recent 6mths resale t...  2018-10-20  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create bigram\n",
    "# data_words = df_assigned[\"clean_text\"].tolist()\n",
    "# bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "# bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# def make_bigrams(texts):\n",
    "#     output = []\n",
    "#     output.append(bigram_mod[texts])\n",
    "#     return output\n",
    "\n",
    "# df_assigned['clean_text'] = df_assigned['clean_text'].apply(lambda x: make_bigrams(x))\n",
    "\n",
    "\n",
    "from nltk.util import ngrams\n",
    "def make_bigrams(texts):\n",
    "    texts = [\"_\".join(w) for w in ngrams(texts, 2)]\n",
    "    return texts\n",
    "\n",
    "df_assigned['clean_text'] = df_assigned['clean_text'].apply(lambda x: make_bigrams(x))\n",
    "df_assigned.head()\n",
    "# lemmatization & noun extraction \n",
    "\n",
    "def lemmatization(text, allowed_postags=['NOUN']): \n",
    "    texts_out = []\n",
    "    \n",
    "    doc = nlp(' '.join(str(v) for v in text)) \n",
    "    print(doc)\n",
    "    text_out = [token.lemma_ for token in doc if token.pos_ in allowed_postags if token.lemma_ != 'nan']\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "df_assigned['clean_text'] = df_assigned['clean_text'].apply(lambda x: lemmatization(x))\n",
    "\n",
    "df_assigned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cfbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed texts together\n",
    "description_str = ''\n",
    "for word_list in df_assigned['clean_text'].values:\n",
    "    try:\n",
    "        description_str += ' '.join(row for row in word_list) + ' '\n",
    "    except:\n",
    "        print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stop_words = stopwords.words('english')\n",
    "# new_stop_words += ['recipe','time', 'dish']\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, stopwords = new_stop_words,\n",
    "                      contour_color='steelblue', collocations = False,  random_state=1)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(description_str)\n",
    "\n",
    "# Visualize the word cloud\n",
    "# wordcloud.to_file(\"wordcloud_before_covid.png\")\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafdc37f",
   "metadata": {},
   "source": [
    "# Topic Modeling for Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0390e6",
   "metadata": {},
   "source": [
    "## Create Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ef94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = df_assigned['clean_text'].tolist()\n",
    "print(data_lemmatized[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e770fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Filter out tokens that appear in only 1 documents and appear in more than 90% of the documents\n",
    "id2word.filter_extremes(no_below=2, no_above=0.9)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bdf330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents. Each word gets an id\n",
    "print('Sample word to id mappings:\\n', list(id2word.items())[:50])\n",
    "print()\n",
    "print('Total Vocabulary Size:', len(id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222654a",
   "metadata": {},
   "source": [
    "## Building LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af70cef",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_evaluation_values(corpus, dictionary, k):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=20,\n",
    "                                           per_word_topics=True)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    \n",
    "    return [coherence_model_lda.get_coherence(), perplexity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99ff50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over possible number of topics.\n",
    "topic_param = []\n",
    "coherence_score = []\n",
    "perplexity_score = []\n",
    "\n",
    "for k in range(2, 20):\n",
    "    print('topic number: ', k)\n",
    "    ev = compute_evaluation_values(corpus=corpus, dictionary=id2word, k=k)\n",
    "    coherence_score.append(ev[0])\n",
    "    perplexity_score.append(ev[1])\n",
    "    print('Coherence Score: ', ev[0])\n",
    "    print('Perplexity Score: ', ev[1])\n",
    "    print()\n",
    "    \n",
    "    topic_param.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07966a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show graph\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(topic_param, coherence_score)\n",
    "\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0cefc",
   "metadata": {},
   "source": [
    "### Final LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abaf3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics_des_name = 5\n",
    "\n",
    "lda_model_des_name = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=ntopics_des_name, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=20,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc532091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the Keyword in the topics\n",
    "pprint(lda_model_des_name.print_topics())\n",
    "doc_lda = lda_model_des_name[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_Tuple(tup):  \n",
    "    return(sorted(tup, key = lambda x: x[1], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68290001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_num = []\n",
    "print(lda_model_des_name.get_document_topics(corpus))\n",
    "for n in range(len(df_assigned)):\n",
    "    get_document_topics = lda_model_des_name.get_document_topics(corpus[n])\n",
    "    sorted_doc_topics = Sort_Tuple(get_document_topics)\n",
    "    all_topic = []\n",
    "    for i in sorted_doc_topics:\n",
    "        all_topic.append(i[0])\n",
    "    topic_num.append(all_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b96924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assigned['Topic'] = topic_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb28a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "des_name_final = df_assigned[['Date', 'text', 'clean_text', 'Topic']]\n",
    "des_name_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc602a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# des_name_final.to_csv('description_name_features_final_all_noun.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f3017",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "perplexity = lda_model_des_name.log_perplexity(corpus)\n",
    "print('Perplexity: ', perplexity)  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_des_name, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ec288",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# # feed the LDA model into the pyLDAvis instance\n",
    "# lda_viz = gensimvis.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428df65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model_des_name, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140c38b",
   "metadata": {},
   "source": [
    "## Word Count of Topic Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36985c17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "topics = lda_model_des_name.show_topics(num_topics=ntopics_des_name, formatted=False)\n",
    "data_flat = [w for w_list in data_lemmatized for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10,15), dpi=160,facecolor='white')\n",
    "cols = ['#008080', '#A52A2A', '#DC143C', '#800000', '#006400', '#556b2f', '#002366', '#ff8c00', '#FF1493', '#9400D3',\n",
    "        '#ba55d3', '#b8860b', '#C71585', '#00ff7f', '#00004C', '#00008B', '#B8860B', '#DEB887', '#8A2BE2', '#1b364a']\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i > ntopics_des_name - 1: # break when all topics are shown\n",
    "        break\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)   \n",
    "plt.savefig('lda_topics_before_bigram.png',facecolor=fig.get_facecolor(), transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a977fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jonathan, [24/10/2022 3:27 PM]\n",
    "# cv1 = CountVectorizer(min_df=3, stop_words='english', ngram_range= (1,2))\n",
    "# cv1_vect = cv1.fit_transform(tweet_corpus)\n",
    "# print(cv1.get_feature_names())\n",
    "\n",
    "# jonathan, [24/10/2022 3:27 PM]\n",
    "# def process_text(documents):\n",
    "#     processed_tokens = []\n",
    "#     doc_tokens = []\n",
    "#     for doc in documents:\n",
    "#         doc = doc.lower()  # change all characters to lower case\n",
    "#         doc = remove_urls(doc) # remove URLs\n",
    "#         doc_tokens = retokenizer.tokenize(doc) # remove punctuations and tokenize the documents\n",
    "#         doc_tokens = [token for token in doc_tokens if token not in stop_words] # remove stop words\n",
    "#         doc_tokens = [wnl.lemmatize(token) for token in doc_tokens] # lemmatize the words\n",
    "#         processed_tokens.append(doc_tokens) # save the pre-processed tokens to a variable called processed_tokens\n",
    "#     return processed_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
