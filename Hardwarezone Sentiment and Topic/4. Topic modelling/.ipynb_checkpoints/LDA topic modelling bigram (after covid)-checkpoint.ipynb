{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8297fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import re \n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim for topic modeling\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import matutils, models\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import scipy.sparse\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\" , disable=['parser', 'ner'])\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca87f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
    "#!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('aftercovid.csv')\n",
    "df[\"Year\"] = df[\"Date\"].str.split(\"-\").str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0265c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1eea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assigned = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assigned = df_assigned.reset_index(drop=True)\n",
    "df_assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332fbd19",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English words\n",
    "\n",
    "# remove punc \n",
    "df_assigned['clean_text']  = df_assigned['join_clean_text'].map(lambda x: re.sub(\"[^A-Za-z0-9]+\",\" \", (x)))\n",
    "# lower case\n",
    "df_assigned['clean_text']  = df_assigned['clean_text'].apply(lambda x: x.lower())\n",
    "# tokenize\n",
    "df_assigned['clean_text']  = [word_tokenize(row) for row in df_assigned['clean_text']]\n",
    "# remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "new_stop_words = ['u','m','lol','condo','said','also',\"click\",\"expand\",\"http\",\"www\",\"com\",\"https\",'from', 'subject', 're', 'edu', 'use','r']\n",
    "stop_words.extend(new_stop_words)\n",
    "def remove_stopwords(text):\n",
    "    return [w for w in text if w not in stop_words]\n",
    "df_assigned['clean_text']  = df_assigned['clean_text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# create bigram\n",
    "def make_bigrams(texts):\n",
    "    texts = [\"_\".join(w) for w in ngrams(texts, 2)]\n",
    "    return texts\n",
    "\n",
    "data_words = df_assigned[\"clean_text\"].tolist()\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "df_assigned['clean_text'] = df_assigned['clean_text'].apply(lambda x: make_bigrams(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoided lemmatization for bigram as the results are more interesting without generating words like 4rooms within1km which are not included in the allowed postags\n",
    "# lemmatization\n",
    "# def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     texts_out = []\n",
    "#     print(texts[0])\n",
    "#     doc = nlp(' '.join(str(v) for v in texts)) \n",
    "#     for token in doc:\n",
    "#         if token.pos_ in allowed_postags:\n",
    "#             texts_out.append(token.lemma_)\n",
    "\n",
    "#     return texts_out\n",
    "\n",
    "# df_assigned['clean_text'] = df_assigned['clean_text'].head().apply(lambda x: lemmatization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65cfbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed texts together\n",
    "description_str = ''\n",
    "for word_list in df_assigned['clean_text'].values:\n",
    "    try:\n",
    "        description_str += ' '.join(row for row in word_list) + ' '\n",
    "    except:\n",
    "        print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stop_words = stopwords.words('english')\n",
    "# new_stop_words += ['recipe','time', 'dish']\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, stopwords = new_stop_words,\n",
    "                      contour_color='steelblue', collocations = False,  random_state=1)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(description_str)\n",
    "\n",
    "# Visualize the word cloud\n",
    "# wordcloud.to_file(\"wordcloud_before_covid.png\")\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafdc37f",
   "metadata": {},
   "source": [
    "# Topic Modeling for Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0390e6",
   "metadata": {},
   "source": [
    "## Create Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ef94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = df_assigned['clean_text'].tolist()\n",
    "print(data_lemmatized[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e770fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Filter out tokens that appear in only 1 documents and appear in more than 90% of the documents\n",
    "id2word.filter_extremes(no_below=2, no_above=0.9)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bdf330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents. Each word gets an id\n",
    "print('Sample word to id mappings:\\n', list(id2word.items())[:50])\n",
    "print()\n",
    "print('Total Vocabulary Size:', len(id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222654a",
   "metadata": {},
   "source": [
    "## Building LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af70cef",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_evaluation_values(corpus, dictionary, k):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=20,\n",
    "                                           per_word_topics=True)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    \n",
    "    return [coherence_model_lda.get_coherence(), perplexity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99ff50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over possible number of topics.\n",
    "topic_param = []\n",
    "coherence_score = []\n",
    "perplexity_score = []\n",
    "\n",
    "for k in range(2, 20):\n",
    "    print('topic number: ', k)\n",
    "    ev = compute_evaluation_values(corpus=corpus, dictionary=id2word, k=k)\n",
    "    coherence_score.append(ev[0])\n",
    "    perplexity_score.append(ev[1])\n",
    "    print('Coherence Score: ', ev[0])\n",
    "    print('Perplexity Score: ', ev[1])\n",
    "    print()\n",
    "    \n",
    "    topic_param.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07966a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show graph\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(topic_param, coherence_score)\n",
    "\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0cefc",
   "metadata": {},
   "source": [
    "### Final LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abaf3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics_des_name = 10\n",
    "\n",
    "lda_model_des_name = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=ntopics_des_name, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=20,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc532091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the Keyword in the topics\n",
    "pprint(lda_model_des_name.print_topics())\n",
    "doc_lda = lda_model_des_name[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_Tuple(tup):  \n",
    "    return(sorted(tup, key = lambda x: x[1], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68290001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_num = []\n",
    "print(lda_model_des_name.get_document_topics(corpus))\n",
    "for n in range(len(df_assigned)):\n",
    "    get_document_topics = lda_model_des_name.get_document_topics(corpus[n])\n",
    "    sorted_doc_topics = Sort_Tuple(get_document_topics)\n",
    "    all_topic = []\n",
    "    for i in sorted_doc_topics:\n",
    "        all_topic.append(i[0])\n",
    "    topic_num.append(all_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b96924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assigned['Topic'] = topic_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb28a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "des_name_final = df_assigned[['Date', 'text', 'clean_text', 'Topic']]\n",
    "des_name_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc602a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# des_name_final.to_csv('description_name_features_final_all_noun.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f3017",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "perplexity = lda_model_des_name.log_perplexity(corpus)\n",
    "print('Perplexity: ', perplexity)  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_des_name, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ec288",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# # feed the LDA model into the pyLDAvis instance\n",
    "# lda_viz = gensimvis.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428df65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model_des_name, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140c38b",
   "metadata": {},
   "source": [
    "## Word Count of Topic Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36985c17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "topics = lda_model_des_name.show_topics(num_topics=ntopics_des_name, formatted=False)\n",
    "data_flat = [w for w_list in data_lemmatized for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(5, 2, figsize=(10,15), dpi=160,facecolor='white')\n",
    "cols = ['#008080', '#A52A2A', '#DC143C', '#800000', '#006400', '#556b2f', '#002366', '#ff8c00', '#FF1493', '#9400D3',\n",
    "        '#ba55d3', '#b8860b', '#C71585', '#00ff7f', '#00004C', '#00008B', '#B8860B', '#DEB887', '#8A2BE2', '#1b364a']\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i > ntopics_des_name - 1: # break when all topics are shown\n",
    "        break\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)   \n",
    "plt.savefig('lda_topics_after_bigram.png',facecolor=fig.get_facecolor(), transparent=True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
