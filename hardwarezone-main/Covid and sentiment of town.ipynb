{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f7e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim for topic modeling\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import matutils, models\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# word by freq \n",
    "# wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726bc266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shawn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shawn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import scipy.sparse\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6620d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combined_hwz.csv', names=['Drop','ThreadName', 'ThreadLink', 'User','UserLink','Rank','Date','text']) \n",
    "df.head() #Filter by dates and then keep text?\n",
    "\n",
    "df = df[[\"Date\",\"text\"]]\n",
    "\n",
    "df[\"Year\"] = df[\"Date\"].str.split(\"-\").str[2]\n",
    "\n",
    "# drop \n",
    "# groupby year \n",
    "# LDA \n",
    "# bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6455988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2022 = df[df[\"Year\"] == \"22\"][[\"Date\",\"text\"]]\n",
    "# df2021 = df[df[\"Year\"] == \"21\"][[\"Date\",\"text\"]]\n",
    "# df2020 = df[df[\"Year\"] == \"20\"][[\"Date\",\"text\"]]\n",
    "# df2019 = df[df[\"Year\"] == \"19\"][[\"Date\",\"text\"]]\n",
    "# df2018 = df[df[\"Year\"] == \"18\"][[\"Date\",\"text\"]]\n",
    "# df2017 = df[df[\"Year\"] == \"17\"][[\"Date\",\"text\"]]\n",
    "# df2016 = df[df[\"Year\"] == \"16\"][[\"Date\",\"text\"]]\n",
    "\n",
    "# df2021.to_csv(\"2021.csv\",index=False)\n",
    "# df2020.to_csv(\"2020.csv\",index=False)\n",
    "# df2019.to_csv(\"2019.csv\",index=False)\n",
    "# df2018.to_csv(\"2018.csv\",index=False)\n",
    "# df2017.to_csv(\"2017.csv\",index=False)\n",
    "# df2016.to_csv(\"2016.csv\",index=False)\n",
    "\n",
    "df_assigned = df\n",
    "df_assigned = df_assigned.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc5a8e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>Year</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>join_clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26-Nov-20</td>\n",
       "      <td>Looking to replace 3x aircons for our HDB flat...</td>\n",
       "      <td>20</td>\n",
       "      <td>[looking, replace, 3x, aircons, hdb, flat, nor...</td>\n",
       "      <td>looking replace 3x aircons hdb flat normal hdb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26-Nov-20</td>\n",
       "      <td>Something just occured to me, do you think tha...</td>\n",
       "      <td>20</td>\n",
       "      <td>[something, occured, think, need, pay, extra, ...</td>\n",
       "      <td>something occured think need pay extra special...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26-Nov-20</td>\n",
       "      <td>A freehold semi-detached house at 61 Jalan Kel...</td>\n",
       "      <td>20</td>\n",
       "      <td>[freehold, semi, detached, house, 61, jalan, k...</td>\n",
       "      <td>freehold semi detached house 61 jalan kelawar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24-Nov-20</td>\n",
       "      <td>Need some advice here from the property gurus ...</td>\n",
       "      <td>20</td>\n",
       "      <td>[need, advice, property, gurus, currently, hol...</td>\n",
       "      <td>need advice property gurus currently hold resa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24-Nov-20</td>\n",
       "      <td>Any grants taken will go back to the CPF accou...</td>\n",
       "      <td>20</td>\n",
       "      <td>[grants, taken, go, back, cpf, account, never,...</td>\n",
       "      <td>grants taken go back cpf account never back hd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315552</th>\n",
       "      <td>26-Nov-20</td>\n",
       "      <td>call me a stereotype, if possible i will not l...</td>\n",
       "      <td>20</td>\n",
       "      <td>[call, stereotype, possible, live, near, renta...</td>\n",
       "      <td>call stereotype possible live near rental flat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315553</th>\n",
       "      <td>26-Nov-20</td>\n",
       "      <td>Rental flat got many problem one. I will not s...</td>\n",
       "      <td>20</td>\n",
       "      <td>[rental, flat, got, many, problem, one, select...</td>\n",
       "      <td>rental flat got many problem one select bto to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315554</th>\n",
       "      <td>26-Nov-20</td>\n",
       "      <td>the estate across the road to mine has rental....</td>\n",
       "      <td>20</td>\n",
       "      <td>[estate, across, road, mine, rental, basically...</td>\n",
       "      <td>estate across road mine rental basically rubbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315555</th>\n",
       "      <td>26-Nov-20</td>\n",
       "      <td>halfnode said:\\nthe estate across the road to ...</td>\n",
       "      <td>20</td>\n",
       "      <td>[halfnode, estate, across, road, mine, rental,...</td>\n",
       "      <td>halfnode estate across road mine rental basica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315556</th>\n",
       "      <td>26-Nov-20</td>\n",
       "      <td>Looking to replace 3x aircons for our HDB flat...</td>\n",
       "      <td>20</td>\n",
       "      <td>[looking, replace, 3x, aircons, hdb, flat, nor...</td>\n",
       "      <td>looking replace 3x aircons hdb flat normal hdb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>315557 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                                               text Year  \\\n",
       "0       26-Nov-20  Looking to replace 3x aircons for our HDB flat...   20   \n",
       "1       26-Nov-20  Something just occured to me, do you think tha...   20   \n",
       "2       26-Nov-20  A freehold semi-detached house at 61 Jalan Kel...   20   \n",
       "3       24-Nov-20  Need some advice here from the property gurus ...   20   \n",
       "4       24-Nov-20  Any grants taken will go back to the CPF accou...   20   \n",
       "...           ...                                                ...  ...   \n",
       "315552  26-Nov-20  call me a stereotype, if possible i will not l...   20   \n",
       "315553  26-Nov-20  Rental flat got many problem one. I will not s...   20   \n",
       "315554  26-Nov-20  the estate across the road to mine has rental....   20   \n",
       "315555  26-Nov-20  halfnode said:\\nthe estate across the road to ...   20   \n",
       "315556  26-Nov-20  Looking to replace 3x aircons for our HDB flat...   20   \n",
       "\n",
       "                                               clean_text  \\\n",
       "0       [looking, replace, 3x, aircons, hdb, flat, nor...   \n",
       "1       [something, occured, think, need, pay, extra, ...   \n",
       "2       [freehold, semi, detached, house, 61, jalan, k...   \n",
       "3       [need, advice, property, gurus, currently, hol...   \n",
       "4       [grants, taken, go, back, cpf, account, never,...   \n",
       "...                                                   ...   \n",
       "315552  [call, stereotype, possible, live, near, renta...   \n",
       "315553  [rental, flat, got, many, problem, one, select...   \n",
       "315554  [estate, across, road, mine, rental, basically...   \n",
       "315555  [halfnode, estate, across, road, mine, rental,...   \n",
       "315556  [looking, replace, 3x, aircons, hdb, flat, nor...   \n",
       "\n",
       "                                          join_clean_text  \n",
       "0       looking replace 3x aircons hdb flat normal hdb...  \n",
       "1       something occured think need pay extra special...  \n",
       "2       freehold semi detached house 61 jalan kelawar ...  \n",
       "3       need advice property gurus currently hold resa...  \n",
       "4       grants taken go back cpf account never back hd...  \n",
       "...                                                   ...  \n",
       "315552  call stereotype possible live near rental flat...  \n",
       "315553  rental flat got many problem one select bto to...  \n",
       "315554  estate across road mine rental basically rubbi...  \n",
       "315555  halfnode estate across road mine rental basica...  \n",
       "315556  looking replace 3x aircons hdb flat normal hdb...  \n",
       "\n",
       "[315557 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove non-English words\n",
    "\n",
    "# remove punc \n",
    "df_assigned['clean_text']  = df_assigned['text'].map(lambda x: re.sub(\"[^A-Za-z0-9]+\",\" \", str(x)))\n",
    "# lower case\n",
    "df_assigned['clean_text']  = df_assigned['clean_text'].apply(lambda x: x.lower())\n",
    "# tokenize\n",
    "df_assigned['clean_text']  = [word_tokenize(row) for row in df_assigned['clean_text']]\n",
    "# remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "new_stop_words = ['u','m','lol','condo','said','also']\n",
    "stop_words.extend(new_stop_words)\n",
    "def remove_stopwords(text):\n",
    "    return [w for w in text if w not in stop_words]\n",
    "df_assigned['clean_text']  = df_assigned['clean_text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "df_assigned['join_clean_text']  = df_assigned['clean_text'].apply(lambda x: \" \".join(x))\n",
    "df_assigned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36445674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ang mo kio\n",
      "671\n",
      "bedok\n",
      "1813\n",
      "bishan\n",
      "1790\n",
      "bukit batok\n",
      "1633\n",
      "bukit merah\n",
      "713\n",
      "bukit panjang\n",
      "773\n",
      "bukit timah\n",
      "744\n",
      "central area\n",
      "293\n",
      "choa chu kang\n",
      "360\n",
      "clementi\n",
      "2957\n",
      "geylang\n",
      "1925\n",
      "hougang\n",
      "2200\n",
      "jurong east\n",
      "545\n",
      "jurong west\n",
      "668\n",
      "kallang/whampoa\n",
      "0\n",
      "marine parade\n",
      "273\n",
      "pasir ris\n",
      "1895\n",
      "punggol\n",
      "7301\n",
      "queenstown\n",
      "1423\n",
      "sembawang\n",
      "2785\n",
      "sengkang\n",
      "3871\n",
      "serangoon\n",
      "1295\n",
      "tampines\n",
      "5311\n",
      "toa payoh\n",
      "1536\n",
      "woodlands\n",
      "2671\n",
      "yishun\n",
      "3903\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"HDB Final.csv\")\n",
    "town = test[\"town\"].apply(lambda x: x.lower()).unique()\n",
    "\n",
    "for t in town:\n",
    "    print(t)\n",
    "    print(df_assigned[df_assigned['join_clean_text'].str.contains(t)].shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78a6cd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ang mo kio', 'bedok', 'bishan', 'bukit batok', 'bukit merah',\n",
       "       'bukit panjang', 'bukit timah', 'central area', 'choa chu kang',\n",
       "       'clementi', 'geylang', 'hougang', 'jurong east', 'jurong west',\n",
       "       'kallang/whampoa', 'marine parade', 'pasir ris', 'punggol',\n",
       "       'queenstown', 'sembawang', 'sengkang', 'serangoon', 'tampines',\n",
       "       'toa payoh', 'woodlands', 'yishun'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "town"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading in all the essentials for data manipulation\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# #load in the NTLK stopwords to remove articles, preposition and other words that are not actionable\n",
    "# from nltk.corpus import stopwords\n",
    "# # This allows to create individual objects from a bog of words\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# # Lemmatizer helps to reduce words to the base form\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# # Ngrams allows to group words in common pairs or trigrams..etc\n",
    "# from nltk import ngrams\n",
    "# # We can use counter to count the objects \n",
    "# from collections import Counter\n",
    "# # This is our visual library\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# new_tokens =[lemmatizer.lemmatize(t) for t in new_tokens]\n",
    "# #counts the words, pairs and trigrams\n",
    "# counted = Counter(new_tokens)\n",
    "# counted_2= Counter(ngrams(new_tokens,2))\n",
    "# counted_3= Counter(ngrams(new_tokens,3))\n",
    "# #creates 3 data frames and returns thems\n",
    "# word_freq = pd.DataFrame(counted.items(),columns=[\"word\",\"frequency\"]).sort_values(by=\"frequency\",ascending=False)\n",
    "# word_pairs =pd.DataFrame(counted_2.items(),columns=[\"pairs\",\"frequency\"]).sort_values(by=\"frequency\",ascending=False)\n",
    "# # trigrams =pd.DataFrame(counted_3.items(),columns=[â€˜trigramsâ€™,â€™frequencyâ€™]).sort_values(by=â€™frequencyâ€™,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be36b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_freq\n",
    "# fig, axes = plt.subplots(2,1,figsize=(8,15))\n",
    "# sns.barplot(ax=axes[0],x='frequency',y='word',data=word_freq.head(30))\n",
    "# sns.barplot(ax=axes[1],x='frequency',y='pairs',data=word_pairs.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lemmatization & noun extraction \n",
    "\n",
    "# def lemmatization(text, allowed_postags=['NOUN']): \n",
    "#     doc = nlp(' '.join(text)) \n",
    "#     text_out = [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "#     return text_out\n",
    "\n",
    "# df_assigned['lem'] = df_assigned['clean_text'].apply(lambda x: lemmatization(x))\n",
    "\n",
    "\n",
    "# df_assigned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f79769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the wordcloud library\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "# # Join the different processed texts together\n",
    "# # description_str = ''\n",
    "# # for word_list in df_assigned['clean_text'].values:\n",
    "# #     description_str += ' '.join(row for row in word_list) + ' '\n",
    "\n",
    "# description_str = \" \".join(new_tokens)\n",
    "\n",
    "\n",
    "# new_stop_words = stopwords.words('english')\n",
    "# # new_stop_words += ['recipe','time', 'dish']\n",
    "\n",
    "# # Create a WordCloud object\n",
    "# wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, stopwords = new_stop_words,\n",
    "#                       contour_color='steelblue', collocations = False,  random_state=1,width=1200, height=800)\n",
    "\n",
    "# # Generate a word cloud\n",
    "# wordcloud.generate(description_str)\n",
    "\n",
    "# # Visualize the word cloud\n",
    "# wordcloud.to_file(\"test.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
