{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o26zyIErwxci"
      },
      "source": [
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9jQ7RlKw1h6",
        "outputId": "158b04b4-a9c0-43fb-a2a1-8147b284947f"
      },
      "source": [
        "\n",
        "!pip install intel-openmp\n",
        "def pad_sents(sents, pad_token):\n",
        "    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n",
        "    @param sents (list[list[int]]): list of sentences, where each sentence\n",
        "                                    is represented as a list of words\n",
        "    @param pad_token (int): padding token\n",
        "    @returns sents_padded (list[list[int]]): list of sentences where sentences shorter\n",
        "        than the max length sentence are padded out with the pad_token, such that\n",
        "        each sentences in the batch now has equal length.\n",
        "        Output shape: (batch_size, max_sentence_length)\n",
        "    \"\"\"\n",
        "    sents_padded = []\n",
        "\n",
        "    max_len = max(len(s) for s in sents)\n",
        "    batch_size = len(sents)\n",
        "\n",
        "    for s in sents:\n",
        "        padded = [pad_token] * max_len\n",
        "        padded[:len(s)] = s\n",
        "        sents_padded.append(padded)\n",
        "\n",
        "    return sents_padded\n",
        "\n",
        "def sents_to_tensor(tokenizer, sents, device):\n",
        "    \"\"\"\n",
        "    :param tokenizer: BertTokenizer\n",
        "    :param sents: list[str], list of sentences (NOTE: untokenized, continuous sentences), reversely sorted\n",
        "    :param device: torch.device\n",
        "    :return: sents_tensor: torch.Tensor, shape(batch_size, max_sent_length), reversely sorted\n",
        "    :return: masks_tensor: torch.Tensor, shape(batch_size, max_sent_length), reversely sorted\n",
        "    :return: sents_lengths: torch.Tensor, shape(batch_size), reversely sorted\n",
        "    \"\"\"\n",
        "    tokens_list = [tokenizer.tokenize(sent) for sent in sents]\n",
        "    sents_lengths = [len(tokens) for tokens in tokens_list]\n",
        "    # tokens_sents_zip = zip(tokens_list, sents_lengths)\n",
        "    # tokens_sents_zip = sorted(tokens_sents_zip, key=lambda x: x[1], reverse=True)\n",
        "    # tokens_list, sents_lengths = zip(*tokens_sents_zip)\n",
        "    tokens_list_padded = pad_sents(tokens_list, '[PAD]')\n",
        "    sents_lengths = torch.tensor(sents_lengths, device=device)\n",
        "\n",
        "    masks = []\n",
        "    for tokens in tokens_list_padded:\n",
        "        mask = [0 if token=='[PAD]' else 1 for token in tokens]\n",
        "        masks.append(mask)\n",
        "    masks_tensor = torch.tensor(masks, dtype=torch.long, device=device)\n",
        "    tokens_id_list = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokens_list_padded]\n",
        "    sents_tensor = torch.tensor(tokens_id_list, dtype=torch.long, device=device)\n",
        "\n",
        "    return sents_tensor, masks_tensor, sents_lengths"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.7/dist-packages (2021.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ICc6cp_w391",
        "outputId": "8244ce69-f2f3-4df4-8530-0206836fae39"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAct4lmPw7dF"
      },
      "source": [
        "# Define the sentiment classification model\n",
        "\n",
        "class SentimentClassifierModel(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_config, device, n_class):\n",
        "        \"\"\"\n",
        "        :param bert_config: str, BERT configuration description\n",
        "        :param device: torch.device\n",
        "        :param n_class: int\n",
        "        \"\"\"\n",
        "\n",
        "        super(SentimentClassifierModel, self).__init__()\n",
        "\n",
        "        self.n_class = n_class\n",
        "        self.bert_config = bert_config\n",
        "        self.bert = BertForSequenceClassification.from_pretrained(self.bert_config, num_labels=self.n_class)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.bert_config)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, sents):\n",
        "        \"\"\"\n",
        "        :param sents: list[str], list of sentences (NOTE: untokenized, continuous sentences)\n",
        "        :return: pre_softmax, torch.tensor of shape (batch_size, n_class)\n",
        "        \"\"\"\n",
        "\n",
        "        sents_tensor, masks_tensor, sents_lengths = sents_to_tensor(self.tokenizer, sents, self.device)\n",
        "        pre_softmax = self.bert(input_ids=sents_tensor, attention_mask=masks_tensor)\n",
        "\n",
        "        return pre_softmax\n",
        "\n",
        "    @staticmethod\n",
        "    def load(model_path: str, device):\n",
        "        \"\"\" Load the model from a file.\n",
        "        @param model_path (str): path to model\n",
        "        @return model (nn.Module): model with saved parameters\n",
        "        \"\"\"\n",
        "        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "        args = params['args']\n",
        "        model = SentimentClassifierModel(device=device, **args)\n",
        "        model.load_state_dict(params['state_dict'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\" Save the model to a file.\n",
        "        @param path (str): path to the model\n",
        "        \"\"\"\n",
        "        print('save model parameters to [%s]' % path, file=sys.stderr)\n",
        "\n",
        "        params = {\n",
        "            'args': dict(bert_config=self.bert_config, n_class=self.n_class),\n",
        "            'state_dict': self.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(params, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "SH9sdFWlxA80",
        "outputId": "5adce501-e84f-456e-e9b4-f3f847712ca9"
      },
      "source": [
        "import pandas as pd\n",
        "!pip install PyDrive\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(pwd)\n",
        "df= pd.read_csv(\"/content/gdrive/MyDrive/Tweets_Tagging.csv\", encoding='utf-8')\n",
        "# df= pd.read_csv(\"/content/gdrive/MyDrive/CSS/Dataset/Tweets_Tagging.csv\", encoding='utf-8')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (1.12.8)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.27.1)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.26.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.7.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.2.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (54.0.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.12.4)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (20.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.53.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2020.12.5)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>its been already 5years serving under â€œunifo...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>already made up my mind. its time to let go .....</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i miss her so much please meet me soon craving...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i dont deserve this. seriously.</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>not in the mood today. totally</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  its been already 5years serving under â€œunifo...    0.0\n",
              "1  already made up my mind. its time to let go .....   -1.0\n",
              "2  i miss her so much please meet me soon craving...   -1.0\n",
              "3                    i dont deserve this. seriously.   -1.0\n",
              "4                     not in the mood today. totally   -1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNq-FkqbNXsY",
        "outputId": "f3a3d423-3d66-4594-8f00-609380d50775"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "dsnBdH0LjBvm",
        "outputId": "283ce6a2-a438-48f0-d947-f0ad83cca019"
      },
      "source": [
        "list_of_text = []\n",
        "for i in df['text']:\n",
        "    i = str(i)\n",
        "    list_of_text.append(i)\n",
        "df[\"text\"] = list_of_text\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>its been already 5years serving under â€œunifo...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>already made up my mind. its time to let go .....</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i miss her so much please meet me soon craving...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i dont deserve this. seriously.</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>not in the mood today. totally</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1803</th>\n",
              "      <td>English is one of the most problematic languag...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1804</th>\n",
              "      <td>@theViscountBP I identify with Johor and Wilay...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1805</th>\n",
              "      <td>@addicted_marv Sure when?</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1806</th>\n",
              "      <td>@AzShah_ Yes</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1807</th>\n",
              "      <td>Baby we know it's for the dick.</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1808 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  label\n",
              "0     its been already 5years serving under â€œunifo...    0.0\n",
              "1     already made up my mind. its time to let go .....   -1.0\n",
              "2     i miss her so much please meet me soon craving...   -1.0\n",
              "3                       i dont deserve this. seriously.   -1.0\n",
              "4                        not in the mood today. totally   -1.0\n",
              "...                                                 ...    ...\n",
              "1803  English is one of the most problematic languag...    0.0\n",
              "1804  @theViscountBP I identify with Johor and Wilay...    0.0\n",
              "1805                          @addicted_marv Sure when?    0.0\n",
              "1806                                       @AzShah_ Yes    0.0\n",
              "1807                    Baby we know it's for the dick.    0.0\n",
              "\n",
              "[1808 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "bTg4SUgwkmQ3",
        "outputId": "579056c9-9f68-4074-b6d1-c7c440c9a1e3"
      },
      "source": [
        "df.text = [''.join([i if ord(i) < 128 else '' for i in df.text]) for text in df.text]\n",
        "df.text = df.text.str.replace(r'_[\\S]?',r'')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-a56a130c0a88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m128\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'_[\\S]?'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mr''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-a56a130c0a88>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m128\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'_[\\S]?'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mr''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-a56a130c0a88>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m128\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'_[\\S]?'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mr''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ord() expected a character, but string of length 100 found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUyZD1QVYOT7",
        "outputId": "94fa5349-5d7f-4250-d7cb-520e8d1a7c78"
      },
      "source": [
        "df.text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       its been already 5years serving under â€œunifo...\n",
              "1       already made up my mind. its time to let go .....\n",
              "2       i miss her so much please meet me soon craving...\n",
              "3                         i dont deserve this. seriously.\n",
              "4                          not in the mood today. totally\n",
              "                              ...                        \n",
              "1803    English is one of the most problematic languag...\n",
              "1804    @theViscountBP I identify with Johor and Wilay...\n",
              "1805                            @addicted_marv Sure when?\n",
              "1806                                         @AzShah_ Yes\n",
              "1807                      Baby we know it's for the dick.\n",
              "Name: text, Length: 1808, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F2-98G5xPlY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "8e12cc23-f04e-4171-bc71-682796579cb1"
      },
      "source": [
        "# Remove URL, RT, mention(@)\n",
        "\n",
        "df.text = df.text.str.replace(r'http(\\S)+', r'')\n",
        "df.text = df.text.str.replace(r'http ...', r'')\n",
        "df.text = df.text.str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
        "df.text = df.text.str.replace(r'@[\\S]+',r'')\n",
        "\n",
        "# Remove non-ascii words or characters\n",
        "# df.text = [''.join([i if ord(i) < 128 else '' for i in text]) for text in df.text]\n",
        "# df.text = df.text.str.replace(r'_[\\S]?',r'')\n",
        "\n",
        "# Remove extra space\n",
        "df.text = df.text.str.replace(r'[ ]{2, }',r' ')\n",
        "\n",
        "# Remove &, < and >\n",
        "df.text = df.text.str.replace(r'&amp;?',r'and')\n",
        "df.text = df.text.str.replace(r'&lt;',r'<')\n",
        "df.text = df.text.str.replace(r'&gt;',r'>')\n",
        "\n",
        "# Insert space between words and punctuation marks\n",
        "df.text = df.text.str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
        "df.text = df.text.str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
        "\n",
        "# Lowercased and strip\n",
        "df.text = df.text.str.lower()\n",
        "df.text = df.text.str.strip()\n",
        "\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>its been already 5years serving under â € œuni...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>already made up my mind . its time to let go ....</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i miss her so much please meet me soon craving...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i dont deserve this . seriously .</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>not in the mood today . totally</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1803</th>\n",
              "      <td>english is one of the most problematic languag...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1804</th>\n",
              "      <td>i identify with johor and wilayah because my r...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1805</th>\n",
              "      <td>sure when ?</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1806</th>\n",
              "      <td>yes</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1807</th>\n",
              "      <td>baby we know it ' s for the dick .</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1808 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  label\n",
              "0     its been already 5years serving under â € œuni...    0.0\n",
              "1     already made up my mind . its time to let go ....   -1.0\n",
              "2     i miss her so much please meet me soon craving...   -1.0\n",
              "3                     i dont deserve this . seriously .   -1.0\n",
              "4                       not in the mood today . totally   -1.0\n",
              "...                                                 ...    ...\n",
              "1803  english is one of the most problematic languag...    0.0\n",
              "1804  i identify with johor and wilayah because my r...    0.0\n",
              "1805                                        sure when ?    0.0\n",
              "1806                                                yes    0.0\n",
              "1807                 baby we know it ' s for the dick .    0.0\n",
              "\n",
              "[1808 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3SYnjkDxRLl",
        "outputId": "cae76f87-809b-401f-8302-cac5367b4098"
      },
      "source": [
        "df['text_length'] = [len(text.split(' ')) for text in df.text]\n",
        "print(df.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1808, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL9pO9UWxU-7",
        "outputId": "8e357cd7-ecf2-4fbb-d42b-ef2a7de97157"
      },
      "source": [
        "df['BERT_processed_text'] = '[CLS] '+ df.text\n",
        "df['BERT_processed_text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [CLS] its been already 5years serving under â ...\n",
              "1       [CLS] already made up my mind . its time to le...\n",
              "2       [CLS] i miss her so much please meet me soon c...\n",
              "3                 [CLS] i dont deserve this . seriously .\n",
              "4                   [CLS] not in the mood today . totally\n",
              "                              ...                        \n",
              "1803    [CLS] english is one of the most problematic l...\n",
              "1804    [CLS] i identify with johor and wilayah becaus...\n",
              "1805                                    [CLS] sure when ?\n",
              "1806                                            [CLS] yes\n",
              "1807             [CLS] baby we know it ' s for the dick .\n",
              "Name: BERT_processed_text, Length: 1808, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmHcha1lxVBk"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "df['BERT_processed_text_length'] = [len(tokenizer.tokenize(sents)) for sents in df.BERT_processed_text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1p9SJy1WaE8"
      },
      "source": [
        "df['label'] = df['label'].fillna(0).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0IwSRjUxZK7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e515649e-df81-4d8c-8698-8137d308e2cd"
      },
      "source": [
        "df.BERT_processed_text_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       29\n",
              "1       15\n",
              "2       35\n",
              "3        9\n",
              "4        8\n",
              "        ..\n",
              "1803    15\n",
              "1804    17\n",
              "1805     4\n",
              "1806     2\n",
              "1807    11\n",
              "Name: BERT_processed_text_length, Length: 1808, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7GrcNU4xa7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39bcf6fe-64c7-4828-f5ab-4b64bc58f9b7"
      },
      "source": [
        "label_dict = dict()\n",
        "for i, l in enumerate(list(df.label.value_counts().keys())):\n",
        "     label_dict.update({l: i})\n",
        "     print(label_dict)\n",
        "\n",
        "# label_dict\n",
        "df['tweet_labels'] = [label_dict[label] for label in df.label]\n",
        "\n",
        "df['airline_sentiment_label'] = [label_dict[label] for label in df.label]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 0}\n",
            "{0: 0, 1: 1}\n",
            "{0: 0, 1: 1, -1: 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFVOM6Nqxce9",
        "outputId": "59b34df0-be3d-4591-d086-f27dfb3349ca"
      },
      "source": [
        "df.label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       0\n",
              "1      -1\n",
              "2      -1\n",
              "3      -1\n",
              "4      -1\n",
              "       ..\n",
              "1803    0\n",
              "1804    0\n",
              "1805    0\n",
              "1806    0\n",
              "1807    0\n",
              "Name: label, Length: 1808, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-zkazYfxe_J"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgYzMz7oxicG"
      },
      "source": [
        "# Define training params\n",
        "label_names = ['positive', 'negative', 'neutral']\n",
        "model_name = 'st-sentiment'\n",
        "device = torch.device(\"cuda:0\")\n",
        "bert_size = 'bert-base-uncased'\n",
        "\n",
        "train_batch_size = 10 # batch size\n",
        "clip_grad = 1.0 # gradient clipping\n",
        "log_every = 10 # number of mini-batches before logging\n",
        "max_epoch = 20 # max number of epochs\n",
        "max_patience = 3 # number of iterations to wait before decaying learning rate\n",
        "max_num_trial = 3 # number of trials before terminating training\n",
        "lr_decay = 0.5 # learning rate decay\n",
        "lr_bert = 0.00002 # BERT learning rate\n",
        "lr = 0.001 # learning rate\n",
        "valid_niter = 500 # perform validation after n iterations\n",
        "dropout = 0.3 # dropout rate\n",
        "verbose = True\n",
        "\n",
        "prefix = model_name + '_' + bert_size\n",
        "model_save_path = pwd + '/My Drive/Colab Notebooks/' + prefix+'_model.bin'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP01H8bpxkaJ",
        "outputId": "f2a802ff-acce-493f-a536-d0a948d92132"
      },
      "source": [
        "# Split up data into train and validation, where validation is 20% of the dataset\n",
        "training_data,validation_data = train_test_split(df,test_size=0.2,random_state=42)\n",
        "print(len(df), len(training_data), len(validation_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1808 1446 362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "AK5pJBUQEbZ_",
        "outputId": "dfc4330d-cba3-491f-def4-cc64d139c8e2"
      },
      "source": [
        "df=df.copy()\n",
        "\n",
        "df['label'] = df['label'].fillna(0).astype(int)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_length</th>\n",
              "      <th>BERT_processed_text</th>\n",
              "      <th>BERT_processed_text_length</th>\n",
              "      <th>tweet_labels</th>\n",
              "      <th>airline_sentiment_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>its been already 5years serving under â € œuni...</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>[CLS] its been already 5years serving under â ...</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>already made up my mind . its time to let go ....</td>\n",
              "      <td>-1</td>\n",
              "      <td>13</td>\n",
              "      <td>[CLS] already made up my mind . its time to le...</td>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i miss her so much please meet me soon craving...</td>\n",
              "      <td>-1</td>\n",
              "      <td>25</td>\n",
              "      <td>[CLS] i miss her so much please meet me soon c...</td>\n",
              "      <td>35</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i dont deserve this . seriously .</td>\n",
              "      <td>-1</td>\n",
              "      <td>7</td>\n",
              "      <td>[CLS] i dont deserve this . seriously .</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>not in the mood today . totally</td>\n",
              "      <td>-1</td>\n",
              "      <td>7</td>\n",
              "      <td>[CLS] not in the mood today . totally</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1803</th>\n",
              "      <td>english is one of the most problematic languag...</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>[CLS] english is one of the most problematic l...</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1804</th>\n",
              "      <td>i identify with johor and wilayah because my r...</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>[CLS] i identify with johor and wilayah becaus...</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1805</th>\n",
              "      <td>sure when ?</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[CLS] sure when ?</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1806</th>\n",
              "      <td>yes</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[CLS] yes</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1807</th>\n",
              "      <td>baby we know it ' s for the dick .</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>[CLS] baby we know it ' s for the dick .</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1808 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  ...  airline_sentiment_label\n",
              "0     its been already 5years serving under â € œuni...  ...                        0\n",
              "1     already made up my mind . its time to let go ....  ...                        2\n",
              "2     i miss her so much please meet me soon craving...  ...                        2\n",
              "3                     i dont deserve this . seriously .  ...                        2\n",
              "4                       not in the mood today . totally  ...                        2\n",
              "...                                                 ...  ...                      ...\n",
              "1803  english is one of the most problematic languag...  ...                        0\n",
              "1804  i identify with johor and wilayah because my r...  ...                        0\n",
              "1805                                        sure when ?  ...                        0\n",
              "1806                                                yes  ...                        0\n",
              "1807                 baby we know it ' s for the dick .  ...                        0\n",
              "\n",
              "[1808 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c3LLxrAxo31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "431bcded-4b16-4d83-c82d-997c652e3b30"
      },
      "source": [
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "train_label = dict(training_data.label.value_counts())\n",
        "label_max = float(max(train_label.values()))\n",
        "train_label_weight = torch.tensor([label_max/train_label[i] for i in range(len(train_label)-1)], device=device)\n",
        "\n",
        "pp.pprint(train_label_weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-1ad7dc380515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_label_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_max\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_pBgjTrxtON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15db23a4-b960-4ae3-9e89-8c4850670ca8"
      },
      "source": [
        "\n",
        "# Set up model and optimizer\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "model = SentimentClassifierModel(bert_size, device, len(label_names))\n",
        "optimizer = AdamW([\n",
        "        {'params': model.bert.bert.parameters()},\n",
        "        {'params': model.bert.classifier.parameters(), 'lr': float(lr)}\n",
        "    ], lr=float(lr_bert))\n",
        "\n",
        "# model = model.to(device)\n",
        "print('Use device: %s' % device, file=sys.stderr)\n",
        "print('Done! time elapsed %.2f sec' % (time.time() - start_time), file=sys.stderr)\n",
        "print('-' * 80, file=sys.stderr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Use device: cuda:0\n",
            "Done! time elapsed 3.53 sec\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0Ltc6RXxxfJ"
      },
      "source": [
        "# Util functions for training\n",
        "import math\n",
        "import logging\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "import sys\n",
        "from docopt import docopt\n",
        "from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix, \\\n",
        "    f1_score, precision_score, recall_score, roc_auc_score\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('agg')\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def batch_iter(data, batch_size, shuffle=False, bert=None):\n",
        "    \"\"\" Yield batches of sentences and labels reverse sorted by length (largest to smallest).\n",
        "    @param data (dataframe): dataframe with ProcessedText (str) and label (int) columns\n",
        "    @param batch_size (int): batch size\n",
        "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
        "    @param bert (str): whether for BERT training. Values: \"large\", \"base\", None\n",
        "    \"\"\"\n",
        "    batch_num = math.ceil(data.shape[0] / batch_size)\n",
        "    index_array = list(range(data.shape[0]))\n",
        "\n",
        "    if shuffle:\n",
        "        data = data.sample(frac=1)\n",
        "\n",
        "    for i in range(batch_num):\n",
        "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
        "        examples = data.iloc[indices].sort_values(by='BERT_processed_text_length', ascending=False)\n",
        "        sents = list(examples.BERT_processed_text)\n",
        "\n",
        "        targets = list(examples.label.values)\n",
        "        yield sents, targets  # list[list[str]] if not bert else list[str], list[int]\n",
        "        \n",
        "def validation(model, df_val, bert_size, loss_func, device):\n",
        "    \"\"\" validation of model during training.\n",
        "    @param model (nn.Module): the model being trained\n",
        "    @param df_val (dataframe): validation dataset\n",
        "    @param bert_size (str): large or base\n",
        "    @param loss_func(nn.Module): loss function\n",
        "    @param device (torch.device)\n",
        "    @return avg loss value across validation dataset\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    df_val = df_val.sort_values(by='BERT_processed_text_length', ascending=False)\n",
        "\n",
        "    ProcessedText_BERT = list(df_val.BERT_processed_text)\n",
        "    InformationType_label = list(df_val.label)\n",
        "\n",
        "    val_batch_size = 32\n",
        "\n",
        "    n_batch = int(np.ceil(df_val.shape[0]/val_batch_size))\n",
        "\n",
        "    total_loss = 0.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_batch):\n",
        "            sents = ProcessedText_BERT[i*val_batch_size: (i+1)*val_batch_size]\n",
        "            targets = torch.tensor(InformationType_label[i*val_batch_size: (i+1)*val_batch_size],\n",
        "                                   dtype=torch.long, device=device)\n",
        "            batch_size = len(sents)\n",
        "            pre_softmax = model(sents)[0]\n",
        "            batch_loss = loss_func(pre_softmax, targets)\n",
        "            total_loss += batch_loss.item()*batch_size\n",
        "\n",
        "    if was_training:\n",
        "        model.train()\n",
        "\n",
        "    return total_loss/df_val.shape[0]\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, path='cm', cmap=plt.cm.Reds):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    pickle.dump(cm, open(path, 'wb'))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23HzDzpex5XC"
      },
      "source": [
        "# Train\n",
        "\n",
        "model.train()\n",
        "cn_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "torch.save(cn_loss, 'loss_func')  # for later testing\n",
        "\n",
        "# Initialize training variables\n",
        "num_trial = 0\n",
        "train_iter = 0\n",
        "patience = 0\n",
        "cum_loss = 0\n",
        "report_loss = 0\n",
        "cum_examples = report_examples = epoch = 0\n",
        "hist_valid_scores = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aRgFPXUyETy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c41356-2a6a-4b83-8096-0ee230928600"
      },
      "source": [
        "! ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tloss_func  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MiYW99gJuKr",
        "outputId": "bccac109-4bf3-4156-d922-8482d1a23734"
      },
      "source": [
        "train_batch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTJfOjubyG6A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "e2a4c7bf-14d6-4037-856c-d2f62ebbb0cf"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "train_time = begin_time = time.time()\n",
        "print('Begin Maximum Likelihood training...')\n",
        "\n",
        "# model = nn.Linear()  #The sigmoid funtion can also be applied here as model=nn.Sigmoid(nn.Linear())\n",
        "\n",
        "# Training loop\n",
        "while True:\n",
        "    epoch += 1\n",
        "    for sents, targets in batch_iter(training_data, batch_size=train_batch_size, shuffle=True, bert='base'):  # for each epoch\n",
        "        train_iter += 1\n",
        "        optimizer.zero_grad()\n",
        "        batch_size = len(sents)\n",
        "        pre_softmax = model(sents)[0]\n",
        "\n",
        "        # Calculate loss and gradient function\n",
        "        loss = cn_loss(pre_softmax, torch.tensor(targets, dtype=torch.long, device=device))\n",
        "        loss.backward()\n",
        "\n",
        "        # Next step\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_losses_val = loss.item() * batch_size\n",
        "        report_loss += batch_losses_val\n",
        "        cum_loss += batch_losses_val\n",
        "\n",
        "        report_examples += batch_size\n",
        "        cum_examples += batch_size\n",
        "\n",
        "        if train_iter % log_every == 0:\n",
        "            print('epoch %d, iter %d, avg. loss %.2f, '\n",
        "                'cum. examples %d, speed %.2f examples/sec, '\n",
        "                'time elapsed %.2f sec' % (epoch, train_iter,\n",
        "                    report_loss / report_examples,\n",
        "                    cum_examples,\n",
        "                    report_examples / (time.time() - train_time),\n",
        "                    time.time() - begin_time), file=sys.stderr)\n",
        "\n",
        "            train_time = time.time()\n",
        "            report_loss = report_examples = 0.\n",
        "\n",
        "        # perform validation\n",
        "        if train_iter % valid_niter == 0:\n",
        "            print('epoch %d, iter %d, cum. loss %.2f, cum. examples %d' % (epoch, train_iter,\n",
        "                cum_loss / cum_examples,\n",
        "                cum_examples), file=sys.stderr)\n",
        "\n",
        "            cum_loss = cum_examples = 0.\n",
        "\n",
        "            print('begin validation ...', file=sys.stderr)\n",
        "\n",
        "            validation_loss = validation(model, validation_data, bert_size, cn_loss, device)   # dev batch size can be a bit larger\n",
        "\n",
        "            print('validation: iter %d, loss %f' % (train_iter, validation_loss), file=sys.stderr)\n",
        "\n",
        "            is_better = len(hist_valid_scores) == 0 or validation_loss < min(hist_valid_scores)\n",
        "            hist_valid_scores.append(validation_loss)\n",
        "\n",
        "            if is_better:\n",
        "                patience = 0\n",
        "                print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n",
        "\n",
        "                model.save(model_save_path)\n",
        "\n",
        "                # also save the optimizers' state\n",
        "                torch.save(optimizer.state_dict(), model_save_path + '.optim')\n",
        "            elif patience < int(max_patience):\n",
        "                patience += 1\n",
        "                print('hit patience %d' % patience, file=sys.stderr)\n",
        "\n",
        "                if patience == int(max_patience):\n",
        "                    num_trial += 1\n",
        "                    print('hit #%d trial' % num_trial, file=sys.stderr)\n",
        "                    if num_trial == max_num_trial:\n",
        "                        print('early stop!', file=sys.stderr)\n",
        "                        exit(0)\n",
        "\n",
        "                    # decay lr, and restore from previously best checkpoint\n",
        "                    print('load previously best model and decay learning rate to %f%%' %\n",
        "                        (float(lr_decay)*100), file=sys.stderr)\n",
        "\n",
        "                    # load model\n",
        "                    params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n",
        "                    model.load_state_dict(params['state_dict'])\n",
        "                    model = model.to(device)\n",
        "\n",
        "                    print('restore parameters of the optimizers', file=sys.stderr)\n",
        "                    optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n",
        "\n",
        "                    # set new lr\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] *= float(lr_decay)\n",
        "\n",
        "                    # reset patience\n",
        "                    patience = 0\n",
        "\n",
        "            if epoch == int(max_epoch):\n",
        "                print('reached maximum number of epochs!', file=sys.stderr)\n",
        "                exit(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin Maximum Likelihood training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-fb2e41419e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpre_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Calculate loss and gradient function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-571ec22efac9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sents)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \"\"\"\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0msents_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msents_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mpre_softmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msents_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-87d7139218d4>\u001b[0m in \u001b[0;36msents_to_tensor\u001b[0;34m(tokenizer, sents, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# tokens_list, sents_lengths = zip(*tokens_sents_zip)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mtokens_list_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[PAD]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0msents_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "Ij1xrKPhIgHo",
        "outputId": "044ce6d6-08c9-4ce5-ae5c-7e7232a6c482"
      },
      "source": [
        "training_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_length</th>\n",
              "      <th>BERT_processed_text</th>\n",
              "      <th>BERT_processed_text_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>900</th>\n",
              "      <td>do u think it ' s accurate ? ðÿ ˜±</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10</td>\n",
              "      <td>[CLS] do u think it ' s accurate ? ðÿ ˜±</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1800</th>\n",
              "      <td>at first i thought it was the metal wire that ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>47</td>\n",
              "      <td>[CLS] at first i thought it was the metal wire...</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1128</th>\n",
              "      <td>the future of pan - african fintech collaborat...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11</td>\n",
              "      <td>[CLS] the future of pan - african fintech coll...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>964</th>\n",
              "      <td>is it because i malay ?</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6</td>\n",
              "      <td>[CLS] is it because i malay ?</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>thk some of em find wtc too far ... weiyi not ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32</td>\n",
              "      <td>[CLS] thk some of em find wtc too far ... weiy...</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1130</th>\n",
              "      <td>super proud to be part of a team consistently ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>36</td>\n",
              "      <td>[CLS] super proud to be part of a team consist...</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1294</th>\n",
              "      <td>was so excited to try this # vegetarian # chic...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>46</td>\n",
              "      <td>[CLS] was so excited to try this # vegetarian ...</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>looking for a christmas feast this year w / o ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24</td>\n",
              "      <td>[CLS] looking for a christmas feast this year ...</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>checked out  ' s stock of # sangria  in ready ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>40</td>\n",
              "      <td>[CLS] checked out  ' s stock of # sangria  in ...</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1126</th>\n",
              "      <td>also totally subscribe to the idea that a true...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>32</td>\n",
              "      <td>[CLS] also totally subscribe to the idea that ...</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1446 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  ...  BERT_processed_text_length\n",
              "900                  do u think it ' s accurate ? ðÿ ˜±  ...                          12\n",
              "1800  at first i thought it was the metal wire that ...  ...                          50\n",
              "1128  the future of pan - african fintech collaborat...  ...                          13\n",
              "964                             is it because i malay ?  ...                           7\n",
              "394   thk some of em find wtc too far ... weiyi not ...  ...                          45\n",
              "...                                                 ...  ...                         ...\n",
              "1130  super proud to be part of a team consistently ...  ...                          41\n",
              "1294  was so excited to try this # vegetarian # chic...  ...                          52\n",
              "860   looking for a christmas feast this year w / o ...  ...                          27\n",
              "1459  checked out  ' s stock of # sangria  in ready ...  ...                          46\n",
              "1126  also totally subscribe to the idea that a true...  ...                          36\n",
              "\n",
              "[1446 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8byaPk2yHa5"
      },
      "source": [
        "df['label'] = df['label'].fillna(0).astype(int)    import numpy as np\n",
        "    import pickle\n",
        "    from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix, \\\n",
        "    f1_score, precision_score, recall_score, roc_auc_score\n",
        "    import matplotlib\n",
        "    matplotlib.use('agg')\n",
        "    from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSTTa2VmyLD3"
      },
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, path='cm', cmap=plt.cm.Reds):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    pickle.dump(cm, open(path, 'wb'))\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNL-jW6oyOHv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "04b1eecc-4ca5-47cc-d394-4bd28c5c5ccd"
      },
      "source": [
        "    print('load best model...')\n",
        "\n",
        "    model = SentimentClassifierModel.load('/content/gdrive/My Drive/Colab Notebooks/' + prefix + '_model.bin', device)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    df_test = validation_data\n",
        "\n",
        "    df_test = df_test.sort_values(by='BERT_processed_text_length', ascending=False)\n",
        "\n",
        "    test_batch_size = 32\n",
        "\n",
        "    n_batch = int(np.ceil(df_test.shape[0]/test_batch_size))\n",
        "\n",
        "    cn_loss = torch.load('loss_func', map_location=lambda storage, loc: storage).to(device)\n",
        "\n",
        "    ProcessedText_BERT = list(df_test.BERT_processed_text)\n",
        "    InformationType_label = list(df_test.airline_sentiment_label)\n",
        "\n",
        "    test_loss = 0.\n",
        "    prediction = []\n",
        "    prob = []\n",
        "\n",
        "    softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_batch):\n",
        "            sents = ProcessedText_BERT[i*test_batch_size: (i+1)*test_batch_size]\n",
        "            targets = torch.tensor(InformationType_label[i * test_batch_size: (i + 1) * test_batch_size],\n",
        "                                   dtype=torch.long, device=device)\n",
        "            batch_size = 10\n",
        "\n",
        "            pre_softmax = model(sents)[0]\n",
        "            batch_loss = cn_loss(pre_softmax, targets)\n",
        "            test_loss += batch_loss.item()*batch_size\n",
        "            prob_batch = softmax(pre_softmax)\n",
        "            prob.append(prob_batch)\n",
        "\n",
        "            prediction.extend([t.item() for t in list(torch.argmax(prob_batch, dim=1))])\n",
        "\n",
        "    prob = torch.cat(tuple(prob), dim=0)\n",
        "    loss = test_loss/df_test.shape[0]\n",
        "\n",
        "    pickle.dump([label_names[i] for i in prediction], open(prefix+'_test_prediction', 'wb'))\n",
        "    pickle.dump(prob.data.cpu().numpy(), open(prefix + '_test_prediction_prob', 'wb'))\n",
        "\n",
        "    accuracy = accuracy_score(df_test.airline_sentiment_label.values, prediction)\n",
        "    matthews = matthews_corrcoef(df_test.airline_sentiment_label.values, prediction)\n",
        "\n",
        "    precisions = {}\n",
        "    recalls = {}\n",
        "    f1s = {}\n",
        "    aucrocs = {}\n",
        "\n",
        "    for i in range(len(label_names)):\n",
        "        prediction_ = [1 if pred == i else 0 for pred in prediction]\n",
        "        true_ = [1 if label == i else 0 for label in df_test.airline_sentiment_label.values]\n",
        "        f1s.update({label_names[i]: f1_score(true_, prediction_)})\n",
        "        precisions.update({label_names[i]: precision_score(true_, prediction_)})\n",
        "        recalls.update({label_names[i]: recall_score(true_, prediction_)})\n",
        "        aucrocs.update({label_names[i]: roc_auc_score(true_, list(t.item() for t in prob[:, i]))})\n",
        "\n",
        "    metrics_dict = {'loss': loss, 'accuracy': accuracy, 'matthews coef': matthews, 'precision': precisions,\n",
        "                         'recall': recalls, 'f1': f1s, 'aucroc': aucrocs}\n",
        "\n",
        "    pickle.dump(metrics_dict, open(prefix+'_evaluation_metrics', 'wb'))\n",
        "\n",
        "    cm = plot_confusion_matrix(list(df_test.airline_sentiment_label.values), prediction, label_names, normalize=False,\n",
        "                          path=prefix+'_test_confusion_matrix', title='confusion matrix for test dataset')\n",
        "    plt.savefig(prefix+'_test_confusion_matrix', format='png')\n",
        "    cm_norm = plot_confusion_matrix(list(df_test.airline_sentiment_label.values), prediction, label_names, normalize=True,\n",
        "                          path=prefix+'_test normalized_confusion_matrix', title='normalized confusion matrix for test dataset')\n",
        "    plt.savefig(prefix+'_test_normalized_confusion_matrix', format='png')\n",
        "\n",
        "    print('loss: %.2f' % loss)\n",
        "    print('accuracy: %.2f' % accuracy)\n",
        "    print('matthews coef: %.2f' % matthews)\n",
        "    print('-' * 80)\n",
        "    for i in range(len(label_names)):\n",
        "        print('precision score for %s: %.2f' % (label_names[i], precisions[label_names[i]]))\n",
        "        print('recall score for %s: %.2f' % (label_names[i], recalls[label_names[i]]))\n",
        "        print('f1 score for %s: %.2f' % (label_names[i], f1s[label_names[i]]))\n",
        "        print('auc roc score for %s: %.2f' % (label_names[i], aucrocs[label_names[i]]))\n",
        "        print('-' * 80)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load best model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-81c49d1de07e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load best model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentClassifierModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Colab Notebooks/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_model.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-571ec22efac9>\u001b[0m in \u001b[0;36mload\u001b[0;34m(model_path, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msaved\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \"\"\"\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'args'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentClassifierModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/Colab Notebooks/st-sentiment_bert-base-uncased_model.bin'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNzcXgtN4ZZj"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "# %cd /content/drive/My Drive/CSSinsta/\n",
        "# st_df = pandas.read_csv('/content/gdrive/MyDrive/2020tweets.xlsx', index_col=0, encoding='latin-1')\n",
        "# st_df= pd.read_csv(\"/content/gdrive/MyDrive/Tweets_Tagging.csv\", encoding='utf-8')\n",
        "st_df= pd.read_excel(\"/content/gdrive/MyDrive/2020tweets.xlsx\")\n",
        "\n",
        "# df = pd.read_csv('2020tweets.xlsx')\n",
        "st_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cztIzqiUdbXY"
      },
      "source": [
        "st_df['text']=st_df['tweet']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-XBtAw1ePT-"
      },
      "source": [
        "st_df=st_df.drop(['tweet'], axis=1)\n",
        "st_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1ZJVZ_M4k1u"
      },
      "source": [
        "# Remove URL, RT, mention(@)\n",
        "\n",
        "st_df['text'] = st_df.text\n",
        "\n",
        "st_df.text = st_df.text.str.replace(r'http(\\S)+', r'')\n",
        "st_df.text = st_df.text.str.replace(r'http ...', r'')\n",
        "st_df.text = st_df.text.str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
        "st_df.text = st_df.text.str.replace(r'@[\\S]+',r'')\n",
        "\n",
        "# Remove non-ascii words or characters\n",
        "st_df.text = [''.join([i if ord(i) < 128 else '' for i in text]) for text in st_df.text]\n",
        "st_df.text = st_df.text.str.replace(r'_[\\S]?',r'')\n",
        "\n",
        "# Remove extra space\n",
        "st_df.text = st_df.text.str.replace(r'[ ]{2, }',r' ')\n",
        "\n",
        "# Remove &, < and >\n",
        "st_df.text = st_df.text.str.replace(r'&amp;?',r'and')\n",
        "st_df.text = st_df.text.str.replace(r'&lt;',r'<')\n",
        "st_df.text = st_df.text.str.replace(r'&gt;',r'>')\n",
        "\n",
        "# Insert space between words and punctuation marks\n",
        "st_df.text = st_df.text.str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
        "st_df.text = st_df.text.str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
        "\n",
        "# Lowercased and strip\n",
        "st_df.text = st_df.text.str.lower()\n",
        "st_df.text = st_df.text.str.strip()\n",
        "\n",
        "st_df['text_length'] = [len(text.split(' ')) for text in st_df.text]\n",
        "print(st_df.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReWnYhiQdpg2"
      },
      "source": [
        "st_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXNeAJkp4n-n"
      },
      "source": [
        "st_df['BERT_processed_text'] = '[CLS] '+ st_df.text\n",
        "st_df.BERT_processed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBQF28yW4szH"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "st_df['BERT_processed_text_length'] = [len(tokenizer.tokenize(sent)) for sent in st_df.text]\n",
        "st_df.BERT_processed_text_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3ZkqWrX4urj"
      },
      "source": [
        "st_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9AcpN954zax"
      },
      "source": [
        "model = SentimentClassifierModel.load('/content/gdrive/My Drive/Colab Notebooks/' + prefix + '_model.bin', device)\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCFb-TKj43fi"
      },
      "source": [
        "st_df = st_df.sort_values(by='BERT_processed_text_length', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTZSy8_Y45zf"
      },
      "source": [
        "st_df\n",
        "\n",
        "#st_df.drop(st_df.tail(5000).index,inplace=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfsRPC4l49KU"
      },
      "source": [
        "cn_loss = torch.load('loss_func', map_location=lambda storage, loc: storage).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_2jElkR5Bl7"
      },
      "source": [
        "ProcessedText_BERT = list(st_df.BERT_processed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKj6K7c75DXm"
      },
      "source": [
        "ProcessedText_BERT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OZ2sgbg5F2j"
      },
      "source": [
        "softmax = torch.nn.Softmax(dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8AdLkMM5H14"
      },
      "source": [
        "labels = ['negative', 'neutral', 'positive']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWriUq995Lom"
      },
      "source": [
        "sents = ProcessedText_BERT[:2]\n",
        "sents\n",
        "\n",
        "\n",
        "pre_softmax = model(sents)[0]\n",
        "pre_softmax\n",
        "\n",
        "prob = softmax(pre_softmax)\n",
        "prob\n",
        "label_indexes = [t.item() for t in list(torch.argmax(prob, dim=1))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-hvNo9C5QO6"
      },
      "source": [
        "\n",
        "prediction = labels[label_indexes[1]]\n",
        "prediction\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtQ4QBxw70U6"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty0c_l135S6y"
      },
      "source": [
        "\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "  sents = ProcessedText_BERT\n",
        "  pre_softmax = model(sents)[0]\n",
        "  prob = softmax(pre_softmax)\n",
        "  predictions.extend([t.item() for t in list(torch.argmax(prob, dim=1))])\n",
        "print(predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ9f6siX5ZoD"
      },
      "source": [
        "[labels[pred_val] for pred_val in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}