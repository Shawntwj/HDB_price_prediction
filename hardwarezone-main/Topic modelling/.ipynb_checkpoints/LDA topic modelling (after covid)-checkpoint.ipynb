{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8297fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim for topic modeling\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import matutils, models\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# word by freq \n",
    "# wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d749a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shawn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import scipy.sparse\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bca87f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
    "#!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8682b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('aftercovid.csv')\n",
    "df.head() #Filter by dates and then keep text?\n",
    "\n",
    "df = df[[\"Date\",\"text\"]]\n",
    "\n",
    "df[\"Year\"] = df[\"Date\"].str.split(\"-\").str[2]\n",
    "\n",
    "\n",
    "\n",
    "# drop \n",
    "# groupby year \n",
    "# LDA \n",
    "# bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801273ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26-Nov-20</td>\n",
       "      <td>A freehold semi-detached house at 61 Jalan Kel...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23-Nov-20</td>\n",
       "      <td>A 4,822 sq ft unit at luxury condo, Nassim Par...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22-Nov-20</td>\n",
       "      <td>vinz said:\\r\\nWrote to HDB in OCT 2020 to get ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18-Nov-20</td>\n",
       "      <td>Wrote to HDB in OCT 2020 to get a confirmation...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28-Oct-20</td>\n",
       "      <td>Assuming price and location is the same, would...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19357</th>\n",
       "      <td>29-Nov-20</td>\n",
       "      <td>The price of 1 bedder in D19 for example, pric...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19358</th>\n",
       "      <td>25-Nov-20</td>\n",
       "      <td>angrymonster said:\\n324k to 397k\\n\\nStack 123 ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19359</th>\n",
       "      <td>25-Nov-20</td>\n",
       "      <td>Is it true that top floor units are taller in ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19360</th>\n",
       "      <td>22-Nov-20</td>\n",
       "      <td>glorfindel said:\\nDepends the walls and window...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19361</th>\n",
       "      <td>27-Nov-20</td>\n",
       "      <td>Two adjoining shophouses at 44 &amp; 46 Bukit Paso...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19362 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date                                               text Year\n",
       "0      26-Nov-20  A freehold semi-detached house at 61 Jalan Kel...   20\n",
       "1      23-Nov-20  A 4,822 sq ft unit at luxury condo, Nassim Par...   20\n",
       "2      22-Nov-20  vinz said:\\r\\nWrote to HDB in OCT 2020 to get ...   20\n",
       "3      18-Nov-20  Wrote to HDB in OCT 2020 to get a confirmation...   20\n",
       "4      28-Oct-20  Assuming price and location is the same, would...   20\n",
       "...          ...                                                ...  ...\n",
       "19357  29-Nov-20  The price of 1 bedder in D19 for example, pric...   20\n",
       "19358  25-Nov-20  angrymonster said:\\n324k to 397k\\n\\nStack 123 ...   20\n",
       "19359  25-Nov-20  Is it true that top floor units are taller in ...   20\n",
       "19360  22-Nov-20  glorfindel said:\\nDepends the walls and window...   20\n",
       "19361  27-Nov-20  Two adjoining shophouses at 44 & 46 Bukit Paso...   20\n",
       "\n",
       "[19362 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0265c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20    9276\n",
       "21    7134\n",
       "22    2952\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Year\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce1eea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assigned = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b09e8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assigned = df_assigned.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332fbd19",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e433c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ea3d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English words\n",
    "\n",
    "# remove punc \n",
    "df_assigned['clean_text']  = df_assigned['text'].map(lambda x: re.sub(\"[^A-Za-z0-9]+\",\" \", str(x)))\n",
    "# lower case\n",
    "df_assigned['clean_text']  = df_assigned['clean_text'].apply(lambda x: x.lower())\n",
    "# tokenize\n",
    "df_assigned['clean_text']  = [word_tokenize(row) for row in df_assigned['clean_text']]\n",
    "\n",
    "# remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "def remove_stopwords(text):\n",
    "    return [w for w in text if w not in stop_words]\n",
    "df_assigned['clean_text']  = df_assigned['clean_text'].apply(lambda x: remove_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "945e4764",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b3eff4ce4ced>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdf_assigned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_assigned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4770\u001b[0m         \"\"\"\n\u001b[1;32m-> 4771\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4773\u001b[0m     def _reduce(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[1;31m# self.f is Callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1157\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-b3eff4ce4ced>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdf_assigned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_assigned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-b3eff4ce4ced>\u001b[0m in \u001b[0;36mlemmatization\u001b[1;34m(text, allowed_postags)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtext_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# lemmatization & noun extraction \n",
    "\n",
    "def lemmatization(text, allowed_postags=['NOUN']): \n",
    "    doc = nlp(' '.join(text)) \n",
    "    text_out = [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "    return text_out\n",
    "\n",
    "df_assigned['clean_text'] = df_assigned['clean_text'].apply(lambda x: lemmatization(x))\n",
    "\n",
    "\n",
    "df_assigned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd945ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed texts together\n",
    "description_str = ''\n",
    "for word_list in df_assigned['clean_text'].values:\n",
    "    description_str += ' '.join(row for row in word_list) + ' '\n",
    "\n",
    "description_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stop_words = stopwords.words('english')\n",
    "# new_stop_words += ['recipe','time', 'dish']\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, stopwords = new_stop_words,\n",
    "                      contour_color='steelblue', collocations = False,  random_state=1)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(description_str)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_file(\"wordcloud_hardwarezone_after_covid.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafdc37f",
   "metadata": {},
   "source": [
    "# Topic Modeling for Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0390e6",
   "metadata": {},
   "source": [
    "## Create Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ef94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = df_assigned['clean_text'].tolist()\n",
    "print(data_lemmatized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e770fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Filter out tokens that appear in only 1 documents and appear in more than 90% of the documents\n",
    "id2word.filter_extremes(no_below=2, no_above=0.9)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bdf330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary representation of the documents. Each word gets an id\n",
    "print('Sample word to id mappings:\\n', list(id2word.items())[:50])\n",
    "print()\n",
    "print('Total Vocabulary Size:', len(id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222654a",
   "metadata": {},
   "source": [
    "## Building LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af70cef",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_evaluation_values(corpus, dictionary, k):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=20,\n",
    "                                           per_word_topics=True)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    perplexity = lda_model.log_perplexity(corpus)\n",
    "    \n",
    "    return [coherence_model_lda.get_coherence(), perplexity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99ff50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over possible number of topics.\n",
    "topic_param = []\n",
    "coherence_score = []\n",
    "perplexity_score = []\n",
    "\n",
    "for k in range(2, 20):\n",
    "    print('topic number: ', k)\n",
    "    ev = compute_evaluation_values(corpus=corpus, dictionary=id2word, k=k)\n",
    "    coherence_score.append(ev[0])\n",
    "    perplexity_score.append(ev[1])\n",
    "    print('Coherence Score: ', ev[0])\n",
    "    print('Perplexity Score: ', ev[1])\n",
    "    print()\n",
    "    \n",
    "    topic_param.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07966a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show graph\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(topic_param, coherence_score)\n",
    "\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0cefc",
   "metadata": {},
   "source": [
    "### Final LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abaf3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics_des_name = 5\n",
    "\n",
    "lda_model_des_name = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=ntopics_des_name, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=20,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc532091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the Keyword in the topics\n",
    "pprint(lda_model_des_name.print_topics())\n",
    "doc_lda = lda_model_des_name[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_Tuple(tup):  \n",
    "    return(sorted(tup, key = lambda x: x[1], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68290001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_num = []\n",
    "print(lda_model_des_name.get_document_topics(corpus))\n",
    "for n in range(len(df_assigned)):\n",
    "    get_document_topics = lda_model_des_name.get_document_topics(corpus[n])\n",
    "    sorted_doc_topics = Sort_Tuple(get_document_topics)\n",
    "    all_topic = []\n",
    "    for i in sorted_doc_topics:\n",
    "        all_topic.append(i[0])\n",
    "    topic_num.append(all_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b96924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assigned['Topic'] = topic_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf617faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb28a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "des_name_final = df_assigned[['Date', 'text', 'clean_text', 'Topic']]\n",
    "des_name_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc602a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "des_name_final.to_csv('description_name_features_final_all_noun.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f3017",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb5cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "perplexity = lda_model_des_name.log_perplexity(corpus)\n",
    "print('Perplexity: ', perplexity)  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_des_name, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ec288",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# # feed the LDA model into the pyLDAvis instance\n",
    "# lda_viz = gensimvis.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428df65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model_des_name, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140c38b",
   "metadata": {},
   "source": [
    "## Word Count of Topic Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36985c17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "topics = lda_model_des_name.show_topics(num_topics=ntopics_des_name, formatted=False)\n",
    "data_flat = [w for w_list in data_lemmatized for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(10, 2, figsize=(10,20), dpi=160)\n",
    "cols = ['#008080', '#A52A2A', '#DC143C', '#800000', '#006400', '#556b2f', '#002366', '#ff8c00', '#FF1493', '#9400D3',\n",
    "        '#ba55d3', '#b8860b', '#C71585', '#00ff7f', '#00004C', '#00008B', '#B8860B', '#DEB887', '#8A2BE2', '#1b364a']\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i > ntopics_des_name - 1: # break when all topics are shown\n",
    "        break\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a977fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
