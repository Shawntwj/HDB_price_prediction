{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9fd4eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shawnteo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/shawnteo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# packages: flair\n",
    "import flair\n",
    "\n",
    "# packages: vader\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# packages: textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "import re\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec24b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install flair==0.6\n",
    "# !pip3 install vaderSentiment\n",
    "# !pip3 install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aae707ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>town</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sep 12, 2021</td>\n",
       "      <td>runroadliao said:\\nThe rats are probably more ...</td>\n",
       "      <td>ang mo kio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sep 12, 2021</td>\n",
       "      <td>Worry the day covid infects rats and mice if i...</td>\n",
       "      <td>ang mo kio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sep 12, 2021</td>\n",
       "      <td>The zehzeh jin brave be pest controller</td>\n",
       "      <td>ang mo kio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sep 11, 2021</td>\n",
       "      <td>Must be coming from nearby churches.</td>\n",
       "      <td>ang mo kio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sep 11, 2021</td>\n",
       "      <td>potatoes_are_fluffy said:\\n\\n\\nClick to expand...</td>\n",
       "      <td>ang mo kio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205333</th>\n",
       "      <td>Sep 20, 2010</td>\n",
       "      <td>otacon said:\\nwill the land beside yishun 10 b...</td>\n",
       "      <td>yishun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205334</th>\n",
       "      <td>Sep 21, 2010</td>\n",
       "      <td>Think09 said:\\nI think u misunderstand. DBSS i...</td>\n",
       "      <td>yishun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205335</th>\n",
       "      <td>Oct 4, 2010</td>\n",
       "      <td>Results are out\\n\\nSianz Q no. 774 ... out of ...</td>\n",
       "      <td>yishun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205336</th>\n",
       "      <td>Oct 4, 2010</td>\n",
       "      <td>I believe there will be drop out. Not to worry...</td>\n",
       "      <td>yishun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205337</th>\n",
       "      <td>Oct 4, 2010</td>\n",
       "      <td>Huat ah..\\nfinally i got my dream location..\\n...</td>\n",
       "      <td>yishun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205338 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                date                                               text  \\\n",
       "0       Sep 12, 2021  runroadliao said:\\nThe rats are probably more ...   \n",
       "1       Sep 12, 2021  Worry the day covid infects rats and mice if i...   \n",
       "2       Sep 12, 2021            The zehzeh jin brave be pest controller   \n",
       "3       Sep 11, 2021               Must be coming from nearby churches.   \n",
       "4       Sep 11, 2021  potatoes_are_fluffy said:\\n\\n\\nClick to expand...   \n",
       "...              ...                                                ...   \n",
       "205333  Sep 20, 2010  otacon said:\\nwill the land beside yishun 10 b...   \n",
       "205334  Sep 21, 2010  Think09 said:\\nI think u misunderstand. DBSS i...   \n",
       "205335   Oct 4, 2010  Results are out\\n\\nSianz Q no. 774 ... out of ...   \n",
       "205336   Oct 4, 2010  I believe there will be drop out. Not to worry...   \n",
       "205337   Oct 4, 2010  Huat ah..\\nfinally i got my dream location..\\n...   \n",
       "\n",
       "              town  \n",
       "0       ang mo kio  \n",
       "1       ang mo kio  \n",
       "2       ang mo kio  \n",
       "3       ang mo kio  \n",
       "4       ang mo kio  \n",
       "...            ...  \n",
       "205333      yishun  \n",
       "205334      yishun  \n",
       "205335      yishun  \n",
       "205336      yishun  \n",
       "205337      yishun  \n",
       "\n",
       "[205338 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"posts.csv\", names=[\"forum\",\"thread_title\",\"thread_link\",\"username\",\"userlink\",\"member_status\",\"date\",\"text\",\"town\"])\n",
    "\n",
    "df_assigned = df[[\"date\",\"text\",\"town\"]]\n",
    "\n",
    "df_assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d03a9a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/54/d_ts3l1x0slbw8z6xjtw2ldm0000gn/T/ipykernel_13625/867154602.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_assigned['clean_text']  = df_assigned['text'].map(lambda x: re.sub(\"[^A-Za-z0-9]+\",\" \", str(x)))\n",
      "/var/folders/54/d_ts3l1x0slbw8z6xjtw2ldm0000gn/T/ipykernel_13625/867154602.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_assigned['clean_text']  = df_assigned['clean_text'].apply(lambda x: x.lower())\n",
      "/var/folders/54/d_ts3l1x0slbw8z6xjtw2ldm0000gn/T/ipykernel_13625/867154602.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_assigned['clean_text']  = [word_tokenize(row) for row in df_assigned['clean_text']]\n",
      "/var/folders/54/d_ts3l1x0slbw8z6xjtw2ldm0000gn/T/ipykernel_13625/867154602.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_assigned['clean_text'] = df_assigned['clean_text'].str.replace('\\d+', '')\n",
      "/var/folders/54/d_ts3l1x0slbw8z6xjtw2ldm0000gn/T/ipykernel_13625/867154602.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_assigned['clean_text'] = df_assigned['clean_text'].str.replace('\\d+', '')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_stopwords\u001b[39m(text):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m text \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m---> 17\u001b[0m df_assigned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;241m=\u001b[39m \u001b[43mdf_assigned\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_stopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m df_assigned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoin_clean_text\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;241m=\u001b[39m df_assigned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# take out posts containing these amenities\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_stopwords\u001b[39m(text):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m text \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m---> 17\u001b[0m df_assigned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;241m=\u001b[39m df_assigned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mremove_stopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     19\u001b[0m df_assigned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoin_clean_text\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;241m=\u001b[39m df_assigned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# take out posts containing these amenities\u001b[39;00m\n",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_stopwords\u001b[39m(text):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m text \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Remove non-English words\n",
    "\n",
    "# remove punc \n",
    "df_assigned['clean_text']  = df_assigned['text'].map(lambda x: re.sub(\"[^A-Za-z0-9]+\",\" \", str(x)))\n",
    "# lower case\n",
    "df_assigned['clean_text']  = df_assigned['clean_text'].apply(lambda x: x.lower())\n",
    "# tokenize\n",
    "df_assigned['clean_text']  = [word_tokenize(row) for row in df_assigned['clean_text']]\n",
    "# replace digit \n",
    "df_assigned['clean_text'] = df_assigned['clean_text'].str.replace('\\d+', '')\n",
    "\n",
    "\n",
    "# # remove stopwords\n",
    "# stop_words = stopwords.words('english')\n",
    "# new_stop_words = ['u','m','lol','condo','said','also']\n",
    "# stop_words.extend(new_stop_words)\n",
    "# def remove_stopwords(text):\n",
    "#     return [w for w in text if w not in stop_words]\n",
    "# df_assigned['clean_text']  = df_assigned['clean_text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# df_assigned['join_clean_text']  = df_assigned['clean_text'].apply(lambda x: \" \".join(x))\n",
    "# # take out posts containing these amenities\n",
    "# df_assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27578cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_sentiment(df, cols_lst):\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    for col in cols_lst:\n",
    "        compound = []\n",
    "        \n",
    "        for text in df[col].to_list():\n",
    "            sentiment_dict = sid_obj.polarity_scores(text) # this produces a dictionary, keys: neg, neu, pos, compound\n",
    "            \n",
    "            # only interested in compound\n",
    "            compound.append(sentiment_dict['compound'])\n",
    "        \n",
    "        if len(sentiment_cols) > 1:\n",
    "            column_compound = col + '_compound'\n",
    "            df[column_compound] = compound\n",
    "    \n",
    "    else:\n",
    "        df['compound'] = compound\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "vader_sentiment(df_assigned,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95601ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(tweet):\n",
    "    def getSubjectivity(text):\n",
    "        return TextBlob(text).sentiment.subjectivity\n",
    "  \n",
    "    # Create a function to get the polarity\n",
    "    def getPolarity(text):\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "\n",
    "        #Create two new columns â€˜Subjectivityâ€™ & â€˜Polarityâ€™\n",
    "        tweet[â€˜TextBlob_Subjectivityâ€™] =    tweet[â€˜tweetâ€™].apply(getSubjectivity)\n",
    "        tweet [â€˜TextBlob_Polarityâ€™] = tweet[â€˜tweetâ€™].apply(getPolarity)\n",
    "        \n",
    "    def getAnalysis(score):\n",
    "        if score < 0:\n",
    "            return â€˜Negativeâ€™\n",
    "        elif score == 0:\n",
    "            return â€˜Neutralâ€™\n",
    "        else:\n",
    "            return â€˜Positiveâ€™\n",
    "        tweet [â€˜TextBlob_Analysisâ€™] = tweet  [â€˜TextBlob_Polarityâ€™].apply(getAnalysis )\n",
    "        return tweet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
