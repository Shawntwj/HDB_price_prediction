{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f7e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim for topic modeling\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import matutils, models\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# word by freq \n",
    "# wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726bc266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shawn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shawn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "import scipy.sparse\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6620d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combined_hwz.csv', names=['Drop','ThreadName', 'ThreadLink', 'User','UserLink','Rank','Date','text']) \n",
    "df.head() #Filter by dates and then keep text?\n",
    "\n",
    "df = df[[\"Date\",\"text\"]]\n",
    "\n",
    "df[\"Year\"] = df[\"Date\"].str.split(\"-\").str[2]\n",
    "\n",
    "# drop \n",
    "# groupby year \n",
    "# LDA \n",
    "# bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6455988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2022 = df[df[\"Year\"] == \"22\"][[\"Date\",\"text\"]]\n",
    "# df2021 = df[df[\"Year\"] == \"21\"][[\"Date\",\"text\"]]\n",
    "# df2020 = df[df[\"Year\"] == \"20\"][[\"Date\",\"text\"]]\n",
    "# df2019 = df[df[\"Year\"] == \"19\"][[\"Date\",\"text\"]]\n",
    "# df2018 = df[df[\"Year\"] == \"18\"][[\"Date\",\"text\"]]\n",
    "# df2017 = df[df[\"Year\"] == \"17\"][[\"Date\",\"text\"]]\n",
    "# df2016 = df[df[\"Year\"] == \"16\"][[\"Date\",\"text\"]]\n",
    "\n",
    "# df2021.to_csv(\"2021.csv\",index=False)\n",
    "# df2020.to_csv(\"2020.csv\",index=False)\n",
    "# df2019.to_csv(\"2019.csv\",index=False)\n",
    "# df2018.to_csv(\"2018.csv\",index=False)\n",
    "# df2017.to_csv(\"2017.csv\",index=False)\n",
    "# df2016.to_csv(\"2016.csv\",index=False)\n",
    "\n",
    "df_assigned = df\n",
    "df_assigned = df_assigned.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English words\n",
    "\n",
    "# remove punc \n",
    "df_assigned['clean_text']  = df_assigned['text'].map(lambda x: re.sub(\"[^A-Za-z0-9]+\",\" \", str(x)))\n",
    "# lower case\n",
    "df_assigned['clean_text']  = df_assigned['clean_text'].apply(lambda x: x.lower())\n",
    "# tokenize\n",
    "df_assigned['clean_text']  = [word_tokenize(row) for row in df_assigned['clean_text']]\n",
    "# remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "new_stop_words = ['u','m','lol','condo','said','also']\n",
    "stop_words.extend(new_stop_words)\n",
    "def remove_stopwords(text):\n",
    "    return [w for w in text if w not in stop_words]\n",
    "df_assigned['clean_text']  = df_assigned['clean_text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "df_assigned['join_clean_text']  = df_assigned['clean_text'].apply(lambda x: \" \".join(x))\n",
    "df_assigned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assigned = df_assigned[df_assigned['join_clean_text'].str.contains(\"hawker|mrt|clinic|school|hospital|mall|market|express\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4432e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = []\n",
    "for i in df_assigned[\"clean_text\"].tolist():\n",
    "    new_tokens+= i\n",
    "print(new_tokens[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading in all the essentials for data manipulation\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# #load in the NTLK stopwords to remove articles, preposition and other words that are not actionable\n",
    "# from nltk.corpus import stopwords\n",
    "# # This allows to create individual objects from a bog of words\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# # Lemmatizer helps to reduce words to the base form\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# # Ngrams allows to group words in common pairs or trigrams..etc\n",
    "# from nltk import ngrams\n",
    "# # We can use counter to count the objects \n",
    "# from collections import Counter\n",
    "# # This is our visual library\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# new_tokens =[lemmatizer.lemmatize(t) for t in new_tokens]\n",
    "# #counts the words, pairs and trigrams\n",
    "# counted = Counter(new_tokens)\n",
    "# counted_2= Counter(ngrams(new_tokens,2))\n",
    "# counted_3= Counter(ngrams(new_tokens,3))\n",
    "# #creates 3 data frames and returns thems\n",
    "# word_freq = pd.DataFrame(counted.items(),columns=[\"word\",\"frequency\"]).sort_values(by=\"frequency\",ascending=False)\n",
    "# word_pairs =pd.DataFrame(counted_2.items(),columns=[\"pairs\",\"frequency\"]).sort_values(by=\"frequency\",ascending=False)\n",
    "# # trigrams =pd.DataFrame(counted_3.items(),columns=[‘trigrams’,’frequency’]).sort_values(by=’frequency’,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be36b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_freq\n",
    "# fig, axes = plt.subplots(2,1,figsize=(8,15))\n",
    "# sns.barplot(ax=axes[0],x='frequency',y='word',data=word_freq.head(30))\n",
    "# sns.barplot(ax=axes[1],x='frequency',y='pairs',data=word_pairs.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lemmatization & noun extraction \n",
    "\n",
    "# def lemmatization(text, allowed_postags=['NOUN']): \n",
    "#     doc = nlp(' '.join(text)) \n",
    "#     text_out = [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "#     return text_out\n",
    "\n",
    "# df_assigned['lem'] = df_assigned['clean_text'].apply(lambda x: lemmatization(x))\n",
    "\n",
    "\n",
    "# df_assigned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f79769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the wordcloud library\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "# # Join the different processed texts together\n",
    "# # description_str = ''\n",
    "# # for word_list in df_assigned['clean_text'].values:\n",
    "# #     description_str += ' '.join(row for row in word_list) + ' '\n",
    "\n",
    "# description_str = \" \".join(new_tokens)\n",
    "\n",
    "\n",
    "# new_stop_words = stopwords.words('english')\n",
    "# # new_stop_words += ['recipe','time', 'dish']\n",
    "\n",
    "# # Create a WordCloud object\n",
    "# wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, stopwords = new_stop_words,\n",
    "#                       contour_color='steelblue', collocations = False,  random_state=1,width=1200, height=800)\n",
    "\n",
    "# # Generate a word cloud\n",
    "# wordcloud.generate(description_str)\n",
    "\n",
    "# # Visualize the word cloud\n",
    "# wordcloud.to_file(\"test.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
